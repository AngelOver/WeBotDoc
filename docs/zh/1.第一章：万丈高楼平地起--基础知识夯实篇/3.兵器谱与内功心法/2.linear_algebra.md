---
title: 线性代数：数据世界的"魔法语言"
createTime: 2025/06/06 18:10:00

---

> 💡 **高斯说**：数学是科学的女王，线性代数是数学的王冠。在搜广推领域，线性代数是理解一切"向量化"的基础。

## 🎯 为什么要学线性代数？

在搜索、推荐、广告系统中，几乎所有的数据和模型都可以用"向量"和"矩阵"来描述：
- 用户和物品的Embedding
- 文档-词项的向量空间模型
- 协同过滤中的矩阵分解
- 神经网络的权重和输入

掌握线性代数，就是掌握了数据世界的"魔法语言"！

## 📚 核心概念速成

### 1. 向量与矩阵

- **向量**：一组有序数值，常用于表示特征、Embedding等
  $$\mathbf{x} = [x_1, x_2, ..., x_n]^T$$
- **矩阵**：二维数组，常用于批量数据、变换、相似度计算
  $$A = [a_{ij}]_{m \times n}$$

#### 向量运算
- 加法、数乘、点积（内积）：
  $$\mathbf{a} \cdot \mathbf{b} = \sum_{i=1}^n a_i b_i$$
- 余弦相似度：
  $$\cos(\theta) = \frac{\mathbf{a} \cdot \mathbf{b}}{\|\mathbf{a}\| \|\mathbf{b}\|}$$

#### 矩阵运算
- 乘法：$C = AB$
- 转置：$A^T$
- 逆矩阵：$A^{-1}$（仅方阵且可逆时）

### 2. 特征分解与奇异值分解（SVD）

#### 特征分解（Eigen Decomposition）
- $A\mathbf{v} = \lambda \mathbf{v}$
- $\lambda$为特征值，$\mathbf{v}$为特征向量
- 应用：PCA、谱聚类、图算法

#### 奇异值分解（SVD）
- $A = U\Sigma V^T$
- $U$、$V$为正交矩阵，$\Sigma$为对角矩阵（奇异值）
- 应用：
  - **降维**：主成分分析（PCA）
  - **矩阵分解推荐**：隐语义模型（如SVD++）
  - **文本处理**：LSA（潜在语义分析）

### 3. 主成分分析（PCA）

- **目标**：用最少的维度保留最多的信息
- **步骤**：
  1. 数据中心化
  2. 计算协方差矩阵
  3. 求特征值和特征向量
  4. 选取最大特征值对应的主成分
- **应用**：降维、可视化、去噪

### 4. 稀疏表示与高维空间

- **稀疏向量**：大部分元素为0，常见于倒排索引、One-hot编码
- **高维空间的挑战**：
  - 维度灾难：距离计算失效、存储和计算成本高
  - 解决方案：降维、稀疏化、近似搜索（如LSH）

## 🛠️ 在搜广推中的应用

### 向量空间模型（VSM）
- 文档和查询都表示为向量，相关性用余弦相似度衡量

### Embedding技术
- 用户、物品、词语等都可嵌入到低维向量空间
- 常见方法：Word2Vec、Item2Vec、Graph Embedding

### 协同过滤与矩阵分解
- 用户-物品评分矩阵$R$，分解为$U \times V^T$
- 预测公式：$\hat{r}_{ui} = \mathbf{u}_u^T \mathbf{v}_i$

### 神经网络中的线性层
- $\mathbf{y} = W\mathbf{x} + b$
- 所有深度学习模型的基础运算

## 📖 延伸阅读

::: note 推荐书籍
1. **《线性代数及其应用》** - David C. Lay：经典教材，理论与应用并重
2. **《统计学习方法》** - 李航：机器学习中的线性代数应用
3. **《深度学习》** - Ian Goodfellow：神经网络与线性代数
:::

> 📝 **思考题**：为什么"向量化"是大规模推荐/检索系统的核心？SVD在推荐系统中解决了什么问题？

::: tip 🎉 章节小结
线性代数是数据世界的"魔法语言"。从向量空间到矩阵分解，从Embedding到神经网络，几乎所有现代搜广推算法的底层都离不开线性代数。掌握它，就是掌握了理解和驾驭大规模数据的钥匙。
:::

> **线性代数就像乐高积木——看似简单的块，能拼出无穷复杂的世界。**





