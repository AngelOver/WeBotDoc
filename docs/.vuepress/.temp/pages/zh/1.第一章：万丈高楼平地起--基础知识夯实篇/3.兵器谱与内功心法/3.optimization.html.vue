<template><div><blockquote>
<p>💡 <strong>高斯说</strong>：没有优化，机器学习只是&quot;调参游戏&quot;。在搜广推领域，优化方法是模型训练、排序、广告竞价的灵魂。</p>
</blockquote>
<h2 id="🎯-为什么要学优化" tabindex="-1"><a class="header-anchor" href="#🎯-为什么要学优化"><span>🎯 为什么要学优化？</span></a></h2>
<ul>
<li>搜索排序模型如何找到最优参数？</li>
<li>推荐系统如何高效训练大规模Embedding？</li>
<li>广告竞价如何在预算约束下最大化ROI？</li>
</ul>
<p>所有这些问题的本质，都是&quot;在约束下寻找最优解&quot;。优化方法就是解决这些问题的&quot;内功心法&quot;。</p>
<h2 id="📚-核心概念速成" tabindex="-1"><a class="header-anchor" href="#📚-核心概念速成"><span>📚 核心概念速成</span></a></h2>
<h3 id="_1-优化问题的基本形式" tabindex="-1"><a class="header-anchor" href="#_1-优化问题的基本形式"><span>1. 优化问题的基本形式</span></a></h3>
<ul>
<li><strong>目标函数</strong>：<span v-pre class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold">x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(\mathbf{x})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathbf">x</span><span class="mclose">)</span></span></span></span>，希望最小化或最大化</li>
<li><strong>约束条件</strong>：<span v-pre class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>g</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold">x</mi><mo stretchy="false">)</mo><mo>≤</mo><mn>0</mn><mo separator="true">,</mo><msub><mi>h</mi><mi>j</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold">x</mi><mo stretchy="false">)</mo><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">g_i(\mathbf{x}) \leq 0, h_j(\mathbf{x}) = 0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathbf">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≤</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em;"></span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathbf">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0</span></span></span></span></li>
<li><strong>最优解</strong>：使目标函数取得最优值的<span v-pre class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="bold">x</mi><mo>∗</mo></msup></mrow><annotation encoding="application/x-tex">\mathbf{x}^*</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6887em;"></span><span class="mord"><span class="mord mathbf">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6887em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span></span></span></span></span></span></span></span></li>
</ul>
<h3 id="_2-常见优化算法" tabindex="-1"><a class="header-anchor" href="#_2-常见优化算法"><span>2. 常见优化算法</span></a></h3>
<h4 id="梯度下降法-gradient-descent" tabindex="-1"><a class="header-anchor" href="#梯度下降法-gradient-descent"><span>梯度下降法（Gradient Descent）</span></a></h4>
<ul>
<li><strong>原理</strong>：每次沿目标函数的负梯度方向更新参数</li>
<li><strong>公式</strong>：<p v-pre class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="bold">x</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi mathvariant="bold">x</mi><mi>t</mi></msub><mo>−</mo><mi>η</mi><mi mathvariant="normal">∇</mi><mi>f</mi><mo stretchy="false">(</mo><msub><mi mathvariant="bold">x</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathbf{x}_{t+1} = \mathbf{x}_t - \eta \nabla f(\mathbf{x}_t)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6528em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord mathbf">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathbf">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">η</span><span class="mord">∇</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathbf">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p>
其中<span v-pre class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>η</mi></mrow><annotation encoding="application/x-tex">\eta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">η</span></span></span></span>为学习率</li>
<li><strong>应用</strong>：线性回归、逻辑回归、神经网络训练</li>
</ul>
<h4 id="随机梯度下降-sgd" tabindex="-1"><a class="header-anchor" href="#随机梯度下降-sgd"><span>随机梯度下降（SGD）</span></a></h4>
<ul>
<li><strong>原理</strong>：每次只用一个样本或小批量样本估算梯度，提升效率</li>
<li><strong>优点</strong>：适合大规模数据，易于并行</li>
<li><strong>缺点</strong>：收敛波动大，需要调参</li>
</ul>
<h4 id="动量法-momentum" tabindex="-1"><a class="header-anchor" href="#动量法-momentum"><span>动量法（Momentum）</span></a></h4>
<ul>
<li><strong>原理</strong>：引入&quot;惯性&quot;，加速收敛，减少震荡</li>
<li><strong>公式</strong>：<p v-pre class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="bold">v</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>γ</mi><msub><mi mathvariant="bold">v</mi><mi>t</mi></msub><mo>+</mo><mi>η</mi><mi mathvariant="normal">∇</mi><mi>f</mi><mo stretchy="false">(</mo><msub><mi mathvariant="bold">x</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mspace linebreak="newline"></mspace><msub><mi mathvariant="bold">x</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi mathvariant="bold">x</mi><mi>t</mi></msub><mo>−</mo><msub><mi mathvariant="bold">v</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">\mathbf{v}_{t+1} = \gamma \mathbf{v}_t + \eta \nabla f(\mathbf{x}_t) \\
\mathbf{x}_{t+1} = \mathbf{x}_t - \mathbf{v}_{t+1}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6528em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.016em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.016em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">η</span><span class="mord">∇</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathbf">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.6528em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord mathbf">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathbf">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6528em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.016em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span></span></span></span></span></p>
其中<span v-pre class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span></span></span></span>为动量系数</li>
</ul>
<h4 id="adam优化器" tabindex="-1"><a class="header-anchor" href="#adam优化器"><span>Adam优化器</span></a></h4>
<ul>
<li><strong>原理</strong>：自适应调整每个参数的学习率，结合动量和RMSProp思想</li>
<li><strong>应用</strong>：深度学习模型训练的主流选择</li>
</ul>
<h4 id="牛顿法与拟牛顿法" tabindex="-1"><a class="header-anchor" href="#牛顿法与拟牛顿法"><span>牛顿法与拟牛顿法</span></a></h4>
<ul>
<li><strong>原理</strong>：利用二阶导数（Hessian矩阵）加速收敛</li>
<li><strong>优点</strong>：收敛速度快</li>
<li><strong>缺点</strong>：计算和存储成本高，适合小规模问题</li>
</ul>
<h3 id="_3-正则化与泛化" tabindex="-1"><a class="header-anchor" href="#_3-正则化与泛化"><span>3. 正则化与泛化</span></a></h3>
<ul>
<li><strong>L1正则化</strong>：<span v-pre class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi><mi mathvariant="normal">∥</mi><mi mathvariant="bold">w</mi><msub><mi mathvariant="normal">∥</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">\lambda \|\mathbf{w}\|_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">λ</span><span class="mord">∥</span><span class="mord mathbf" style="margin-right:0.01597em;">w</span><span class="mord"><span class="mord">∥</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，促使参数稀疏，特征选择</li>
<li><strong>L2正则化</strong>：<span v-pre class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi><mi mathvariant="normal">∥</mi><mi mathvariant="bold">w</mi><msubsup><mi mathvariant="normal">∥</mi><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">\lambda \|\mathbf{w}\|_2^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord mathnormal">λ</span><span class="mord">∥</span><span class="mord mathbf" style="margin-right:0.01597em;">w</span><span class="mord"><span class="mord">∥</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.4519em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2481em;"><span></span></span></span></span></span></span></span></span></span>，防止过拟合，参数收缩</li>
<li><strong>早停法</strong>：在验证集性能不再提升时提前终止训练，防止过拟合</li>
</ul>
<h3 id="_4-凸优化与非凸优化" tabindex="-1"><a class="header-anchor" href="#_4-凸优化与非凸优化"><span>4. 凸优化与非凸优化</span></a></h3>
<ul>
<li><strong>凸优化</strong>：目标函数和约束都是凸的，只有一个全局最优解</li>
<li><strong>非凸优化</strong>：可能有多个局部最优，深度学习/推荐系统常见</li>
<li><strong>常用技巧</strong>：多次随机初始化、批量归一化、学习率衰减</li>
</ul>
<h2 id="🛠️-在搜广推中的应用" tabindex="-1"><a class="header-anchor" href="#🛠️-在搜广推中的应用"><span>🛠️ 在搜广推中的应用</span></a></h2>
<h3 id="搜索排序模型训练" tabindex="-1"><a class="header-anchor" href="#搜索排序模型训练"><span>搜索排序模型训练</span></a></h3>
<ul>
<li>RankNet、LambdaMART等排序模型本质上都是优化损失函数</li>
<li>损失函数设计直接影响排序效果</li>
</ul>
<h3 id="推荐系统embedding训练" tabindex="-1"><a class="header-anchor" href="#推荐系统embedding训练"><span>推荐系统Embedding训练</span></a></h3>
<ul>
<li>矩阵分解、Word2Vec、DeepFM等模型都依赖高效优化算法</li>
<li>大规模分布式SGD、异步更新、负采样等工程技巧</li>
</ul>
<h3 id="广告竞价与预算分配" tabindex="-1"><a class="header-anchor" href="#广告竞价与预算分配"><span>广告竞价与预算分配</span></a></h3>
<ul>
<li>预算约束下的最大化ROI问题</li>
<li>常用方法：线性规划、动态规划、贪心算法</li>
</ul>
<h3 id="超参数调优" tabindex="-1"><a class="header-anchor" href="#超参数调优"><span>超参数调优</span></a></h3>
<ul>
<li>学习率、正则化系数、批量大小等都需通过优化实验确定</li>
<li>常用方法：网格搜索、随机搜索、贝叶斯优化</li>
</ul>
<h2 id="📖-延伸阅读" tabindex="-1"><a class="header-anchor" href="#📖-延伸阅读"><span>📖 延伸阅读</span></a></h2>
<div class="hint-container note">
<p class="hint-container-title">推荐书籍</p>
<ol>
<li><strong>《最优化方法》</strong> - 丁同仁：经典优化教材，理论与算法并重</li>
<li><strong>《Convex Optimization》</strong> - Boyd &amp; Vandenberghe：凸优化圣经</li>
<li><strong>《深度学习》</strong> - Ian Goodfellow：深度学习中的优化方法</li>
</ol>
</div>
<blockquote>
<p>📝 <strong>思考题</strong>：为什么深度学习/推荐系统常常陷入局部最优？你如何设计优化流程来提升大规模模型的训练效率？</p>
</blockquote>
<div class="hint-container tip">
<p class="hint-container-title">🎉 章节小结</p>
<p>优化方法是让模型&quot;更优&quot;的秘密武器。从梯度下降到牛顿法，从正则化到超参数调优，优化思想贯穿了搜广推系统的每一个环节。掌握优化，就是掌握了让算法持续进化的钥匙。</p>
</div>
<blockquote>
<p><strong>优化就像修炼内功——看不见却决定了你能走多远。</strong></p>
</blockquote>
</div></template>


