{"content":"<blockquote>\n<p>💡 <strong>高斯说</strong>：没有优化，机器学习只是&quot;调参游戏&quot;。在搜广推领域，优化方法是模型训练、排序、广告竞价的灵魂。</p>\n</blockquote>\n<h2 id=\"🎯-为什么要学优化\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#🎯-为什么要学优化\"><span>🎯 为什么要学优化？</span></a></h2>\n<ul>\n<li>搜索排序模型如何找到最优参数？</li>\n<li>推荐系统如何高效训练大规模Embedding？</li>\n<li>广告竞价如何在预算约束下最大化ROI？</li>\n</ul>\n<p>所有这些问题的本质，都是&quot;在约束下寻找最优解&quot;。优化方法就是解决这些问题的&quot;内功心法&quot;。</p>\n<h2 id=\"📚-核心概念速成\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📚-核心概念速成\"><span>📚 核心概念速成</span></a></h2>\n<h3 id=\"_1-优化问题的基本形式\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_1-优化问题的基本形式\"><span>1. 优化问题的基本形式</span></a></h3>\n<ul>\n<li><strong>目标函数</strong>：<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>f</mi><mo stretchy=\"false\">(</mo><mi mathvariant=\"bold\">x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">f(\\mathbf{x})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord mathbf\">x</span><span class=\"mclose\">)</span></span></span></span>，希望最小化或最大化</li>\n<li><strong>约束条件</strong>：<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>g</mi><mi>i</mi></msub><mo stretchy=\"false\">(</mo><mi mathvariant=\"bold\">x</mi><mo stretchy=\"false\">)</mo><mo>≤</mo><mn>0</mn><mo separator=\"true\">,</mo><msub><mi>h</mi><mi>j</mi></msub><mo stretchy=\"false\">(</mo><mi mathvariant=\"bold\">x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">g_i(\\mathbf{x}) \\leq 0, h_j(\\mathbf{x}) = 0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathbf\">x</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">≤</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0361em;vertical-align:-0.2861em;\"></span><span class=\"mord\">0</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">h</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathbf\">x</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">0</span></span></span></span></li>\n<li><strong>最优解</strong>：使目标函数取得最优值的<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi mathvariant=\"bold\">x</mi><mo>∗</mo></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{x}^*</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6887em;\"></span><span class=\"mord\"><span class=\"mord mathbf\">x</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6887em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mbin mtight\">∗</span></span></span></span></span></span></span></span></span></span></span></li>\n</ul>\n<h3 id=\"_2-常见优化算法\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_2-常见优化算法\"><span>2. 常见优化算法</span></a></h3>\n<h4 id=\"梯度下降法-gradient-descent\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#梯度下降法-gradient-descent\"><span>梯度下降法（Gradient Descent）</span></a></h4>\n<ul>\n<li><strong>原理</strong>：每次沿目标函数的负梯度方向更新参数</li>\n<li><strong>公式</strong>：<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi mathvariant=\"bold\">x</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi mathvariant=\"bold\">x</mi><mi>t</mi></msub><mo>−</mo><mi>η</mi><mi mathvariant=\"normal\">∇</mi><mi>f</mi><mo stretchy=\"false\">(</mo><msub><mi mathvariant=\"bold\">x</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\mathbf{x}_{t+1} = \\mathbf{x}_t - \\eta \\nabla f(\\mathbf{x}_t)\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6528em;vertical-align:-0.2083em;\"></span><span class=\"mord\"><span class=\"mord mathbf\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">t</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathbf\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">η</span><span class=\"mord\">∇</span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathbf\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></span></p>\n其中<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>η</mi></mrow><annotation encoding=\"application/x-tex\">\\eta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">η</span></span></span></span>为学习率</li>\n<li><strong>应用</strong>：线性回归、逻辑回归、神经网络训练</li>\n</ul>\n<h4 id=\"随机梯度下降-sgd\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#随机梯度下降-sgd\"><span>随机梯度下降（SGD）</span></a></h4>\n<ul>\n<li><strong>原理</strong>：每次只用一个样本或小批量样本估算梯度，提升效率</li>\n<li><strong>优点</strong>：适合大规模数据，易于并行</li>\n<li><strong>缺点</strong>：收敛波动大，需要调参</li>\n</ul>\n<h4 id=\"动量法-momentum\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#动量法-momentum\"><span>动量法（Momentum）</span></a></h4>\n<ul>\n<li><strong>原理</strong>：引入&quot;惯性&quot;，加速收敛，减少震荡</li>\n<li><strong>公式</strong>：<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi mathvariant=\"bold\">v</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>γ</mi><msub><mi mathvariant=\"bold\">v</mi><mi>t</mi></msub><mo>+</mo><mi>η</mi><mi mathvariant=\"normal\">∇</mi><mi>f</mi><mo stretchy=\"false\">(</mo><msub><mi mathvariant=\"bold\">x</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo><mspace linebreak=\"newline\"></mspace><msub><mi mathvariant=\"bold\">x</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi mathvariant=\"bold\">x</mi><mi>t</mi></msub><mo>−</mo><msub><mi mathvariant=\"bold\">v</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">\\mathbf{v}_{t+1} = \\gamma \\mathbf{v}_t + \\eta \\nabla f(\\mathbf{x}_t) \\\\\n\\mathbf{x}_{t+1} = \\mathbf{x}_t - \\mathbf{v}_{t+1}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6528em;vertical-align:-0.2083em;\"></span><span class=\"mord\"><span class=\"mord mathbf\" style=\"margin-right:0.01597em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.016em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">t</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7778em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05556em;\">γ</span><span class=\"mord\"><span class=\"mord mathbf\" style=\"margin-right:0.01597em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:-0.016em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">η</span><span class=\"mord\">∇</span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathbf\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span><span class=\"mspace newline\"></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6528em;vertical-align:-0.2083em;\"></span><span class=\"mord\"><span class=\"mord mathbf\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">t</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathbf\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6528em;vertical-align:-0.2083em;\"></span><span class=\"mord\"><span class=\"mord mathbf\" style=\"margin-right:0.01597em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.016em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">t</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span></span></span></span></span></p>\n其中<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>γ</mi></mrow><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05556em;\">γ</span></span></span></span>为动量系数</li>\n</ul>\n<h4 id=\"adam优化器\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#adam优化器\"><span>Adam优化器</span></a></h4>\n<ul>\n<li><strong>原理</strong>：自适应调整每个参数的学习率，结合动量和RMSProp思想</li>\n<li><strong>应用</strong>：深度学习模型训练的主流选择</li>\n</ul>\n<h4 id=\"牛顿法与拟牛顿法\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#牛顿法与拟牛顿法\"><span>牛顿法与拟牛顿法</span></a></h4>\n<ul>\n<li><strong>原理</strong>：利用二阶导数（Hessian矩阵）加速收敛</li>\n<li><strong>优点</strong>：收敛速度快</li>\n<li><strong>缺点</strong>：计算和存储成本高，适合小规模问题</li>\n</ul>\n<h3 id=\"_3-正则化与泛化\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_3-正则化与泛化\"><span>3. 正则化与泛化</span></a></h3>\n<ul>\n<li><strong>L1正则化</strong>：<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>λ</mi><mi mathvariant=\"normal\">∥</mi><mi mathvariant=\"bold\">w</mi><msub><mi mathvariant=\"normal\">∥</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">\\lambda \\|\\mathbf{w}\\|_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">λ</span><span class=\"mord\">∥</span><span class=\"mord mathbf\" style=\"margin-right:0.01597em;\">w</span><span class=\"mord\"><span class=\"mord\">∥</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>，促使参数稀疏，特征选择</li>\n<li><strong>L2正则化</strong>：<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>λ</mi><mi mathvariant=\"normal\">∥</mi><mi mathvariant=\"bold\">w</mi><msubsup><mi mathvariant=\"normal\">∥</mi><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding=\"application/x-tex\">\\lambda \\|\\mathbf{w}\\|_2^2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.0641em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">λ</span><span class=\"mord\">∥</span><span class=\"mord mathbf\" style=\"margin-right:0.01597em;\">w</span><span class=\"mord\"><span class=\"mord\">∥</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141em;\"><span style=\"top:-2.4519em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2481em;\"><span></span></span></span></span></span></span></span></span></span>，防止过拟合，参数收缩</li>\n<li><strong>早停法</strong>：在验证集性能不再提升时提前终止训练，防止过拟合</li>\n</ul>\n<h3 id=\"_4-凸优化与非凸优化\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_4-凸优化与非凸优化\"><span>4. 凸优化与非凸优化</span></a></h3>\n<ul>\n<li><strong>凸优化</strong>：目标函数和约束都是凸的，只有一个全局最优解</li>\n<li><strong>非凸优化</strong>：可能有多个局部最优，深度学习/推荐系统常见</li>\n<li><strong>常用技巧</strong>：多次随机初始化、批量归一化、学习率衰减</li>\n</ul>\n<h2 id=\"🛠️-在搜广推中的应用\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#🛠️-在搜广推中的应用\"><span>🛠️ 在搜广推中的应用</span></a></h2>\n<h3 id=\"搜索排序模型训练\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#搜索排序模型训练\"><span>搜索排序模型训练</span></a></h3>\n<ul>\n<li>RankNet、LambdaMART等排序模型本质上都是优化损失函数</li>\n<li>损失函数设计直接影响排序效果</li>\n</ul>\n<h3 id=\"推荐系统embedding训练\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#推荐系统embedding训练\"><span>推荐系统Embedding训练</span></a></h3>\n<ul>\n<li>矩阵分解、Word2Vec、DeepFM等模型都依赖高效优化算法</li>\n<li>大规模分布式SGD、异步更新、负采样等工程技巧</li>\n</ul>\n<h3 id=\"广告竞价与预算分配\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#广告竞价与预算分配\"><span>广告竞价与预算分配</span></a></h3>\n<ul>\n<li>预算约束下的最大化ROI问题</li>\n<li>常用方法：线性规划、动态规划、贪心算法</li>\n</ul>\n<h3 id=\"超参数调优\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#超参数调优\"><span>超参数调优</span></a></h3>\n<ul>\n<li>学习率、正则化系数、批量大小等都需通过优化实验确定</li>\n<li>常用方法：网格搜索、随机搜索、贝叶斯优化</li>\n</ul>\n<h2 id=\"📖-延伸阅读\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📖-延伸阅读\"><span>📖 延伸阅读</span></a></h2>\n<div class=\"hint-container note\">\n<p class=\"hint-container-title\">推荐书籍</p>\n<ol>\n<li><strong>《最优化方法》</strong> - 丁同仁：经典优化教材，理论与算法并重</li>\n<li><strong>《Convex Optimization》</strong> - Boyd &amp; Vandenberghe：凸优化圣经</li>\n<li><strong>《深度学习》</strong> - Ian Goodfellow：深度学习中的优化方法</li>\n</ol>\n</div>\n<blockquote>\n<p>📝 <strong>思考题</strong>：为什么深度学习/推荐系统常常陷入局部最优？你如何设计优化流程来提升大规模模型的训练效率？</p>\n</blockquote>\n<div class=\"hint-container tip\">\n<p class=\"hint-container-title\">🎉 章节小结</p>\n<p>优化方法是让模型&quot;更优&quot;的秘密武器。从梯度下降到牛顿法，从正则化到超参数调优，优化思想贯穿了搜广推系统的每一个环节。掌握优化，就是掌握了让算法持续进化的钥匙。</p>\n</div>\n<blockquote>\n<p><strong>优化就像修炼内功——看不见却决定了你能走多远。</strong></p>\n</blockquote>\n","env":{"base":"/search-rec-ads-cosmos-explorer/","filePath":"D:/softwore/user/git/work_code/WeBotDoc/docs/zh/1.第一章：万丈高楼平地起--基础知识夯实篇/3.兵器谱与内功心法/3.optimization.md","filePathRelative":"zh/1.第一章：万丈高楼平地起--基础知识夯实篇/3.兵器谱与内功心法/3.optimization.md","frontmatter":{"title":"优化方法：让模型\"更优\"的秘密武器","createTime":"2025/06/06 18:20:00"},"sfcBlocks":{"template":{"type":"template","content":"<template><blockquote>\n<p>💡 <strong>高斯说</strong>：没有优化，机器学习只是&quot;调参游戏&quot;。在搜广推领域，优化方法是模型训练、排序、广告竞价的灵魂。</p>\n</blockquote>\n<h2 id=\"🎯-为什么要学优化\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#🎯-为什么要学优化\"><span>🎯 为什么要学优化？</span></a></h2>\n<ul>\n<li>搜索排序模型如何找到最优参数？</li>\n<li>推荐系统如何高效训练大规模Embedding？</li>\n<li>广告竞价如何在预算约束下最大化ROI？</li>\n</ul>\n<p>所有这些问题的本质，都是&quot;在约束下寻找最优解&quot;。优化方法就是解决这些问题的&quot;内功心法&quot;。</p>\n<h2 id=\"📚-核心概念速成\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📚-核心概念速成\"><span>📚 核心概念速成</span></a></h2>\n<h3 id=\"_1-优化问题的基本形式\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_1-优化问题的基本形式\"><span>1. 优化问题的基本形式</span></a></h3>\n<ul>\n<li><strong>目标函数</strong>：<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>f</mi><mo stretchy=\"false\">(</mo><mi mathvariant=\"bold\">x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">f(\\mathbf{x})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord mathbf\">x</span><span class=\"mclose\">)</span></span></span></span>，希望最小化或最大化</li>\n<li><strong>约束条件</strong>：<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>g</mi><mi>i</mi></msub><mo stretchy=\"false\">(</mo><mi mathvariant=\"bold\">x</mi><mo stretchy=\"false\">)</mo><mo>≤</mo><mn>0</mn><mo separator=\"true\">,</mo><msub><mi>h</mi><mi>j</mi></msub><mo stretchy=\"false\">(</mo><mi mathvariant=\"bold\">x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">g_i(\\mathbf{x}) \\leq 0, h_j(\\mathbf{x}) = 0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathbf\">x</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">≤</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0361em;vertical-align:-0.2861em;\"></span><span class=\"mord\">0</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">h</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathbf\">x</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">0</span></span></span></span></li>\n<li><strong>最优解</strong>：使目标函数取得最优值的<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi mathvariant=\"bold\">x</mi><mo>∗</mo></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{x}^*</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6887em;\"></span><span class=\"mord\"><span class=\"mord mathbf\">x</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6887em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mbin mtight\">∗</span></span></span></span></span></span></span></span></span></span></span></li>\n</ul>\n<h3 id=\"_2-常见优化算法\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_2-常见优化算法\"><span>2. 常见优化算法</span></a></h3>\n<h4 id=\"梯度下降法-gradient-descent\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#梯度下降法-gradient-descent\"><span>梯度下降法（Gradient Descent）</span></a></h4>\n<ul>\n<li><strong>原理</strong>：每次沿目标函数的负梯度方向更新参数</li>\n<li><strong>公式</strong>：<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi mathvariant=\"bold\">x</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi mathvariant=\"bold\">x</mi><mi>t</mi></msub><mo>−</mo><mi>η</mi><mi mathvariant=\"normal\">∇</mi><mi>f</mi><mo stretchy=\"false\">(</mo><msub><mi mathvariant=\"bold\">x</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\mathbf{x}_{t+1} = \\mathbf{x}_t - \\eta \\nabla f(\\mathbf{x}_t)\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6528em;vertical-align:-0.2083em;\"></span><span class=\"mord\"><span class=\"mord mathbf\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">t</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathbf\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">η</span><span class=\"mord\">∇</span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathbf\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></span></p>\n其中<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>η</mi></mrow><annotation encoding=\"application/x-tex\">\\eta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">η</span></span></span></span>为学习率</li>\n<li><strong>应用</strong>：线性回归、逻辑回归、神经网络训练</li>\n</ul>\n<h4 id=\"随机梯度下降-sgd\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#随机梯度下降-sgd\"><span>随机梯度下降（SGD）</span></a></h4>\n<ul>\n<li><strong>原理</strong>：每次只用一个样本或小批量样本估算梯度，提升效率</li>\n<li><strong>优点</strong>：适合大规模数据，易于并行</li>\n<li><strong>缺点</strong>：收敛波动大，需要调参</li>\n</ul>\n<h4 id=\"动量法-momentum\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#动量法-momentum\"><span>动量法（Momentum）</span></a></h4>\n<ul>\n<li><strong>原理</strong>：引入&quot;惯性&quot;，加速收敛，减少震荡</li>\n<li><strong>公式</strong>：<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi mathvariant=\"bold\">v</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>γ</mi><msub><mi mathvariant=\"bold\">v</mi><mi>t</mi></msub><mo>+</mo><mi>η</mi><mi mathvariant=\"normal\">∇</mi><mi>f</mi><mo stretchy=\"false\">(</mo><msub><mi mathvariant=\"bold\">x</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo><mspace linebreak=\"newline\"></mspace><msub><mi mathvariant=\"bold\">x</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi mathvariant=\"bold\">x</mi><mi>t</mi></msub><mo>−</mo><msub><mi mathvariant=\"bold\">v</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">\\mathbf{v}_{t+1} = \\gamma \\mathbf{v}_t + \\eta \\nabla f(\\mathbf{x}_t) \\\\\n\\mathbf{x}_{t+1} = \\mathbf{x}_t - \\mathbf{v}_{t+1}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6528em;vertical-align:-0.2083em;\"></span><span class=\"mord\"><span class=\"mord mathbf\" style=\"margin-right:0.01597em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.016em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">t</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7778em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05556em;\">γ</span><span class=\"mord\"><span class=\"mord mathbf\" style=\"margin-right:0.01597em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:-0.016em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">η</span><span class=\"mord\">∇</span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathbf\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span><span class=\"mspace newline\"></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6528em;vertical-align:-0.2083em;\"></span><span class=\"mord\"><span class=\"mord mathbf\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">t</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathbf\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6528em;vertical-align:-0.2083em;\"></span><span class=\"mord\"><span class=\"mord mathbf\" style=\"margin-right:0.01597em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.016em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">t</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span></span></span></span></span></p>\n其中<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>γ</mi></mrow><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05556em;\">γ</span></span></span></span>为动量系数</li>\n</ul>\n<h4 id=\"adam优化器\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#adam优化器\"><span>Adam优化器</span></a></h4>\n<ul>\n<li><strong>原理</strong>：自适应调整每个参数的学习率，结合动量和RMSProp思想</li>\n<li><strong>应用</strong>：深度学习模型训练的主流选择</li>\n</ul>\n<h4 id=\"牛顿法与拟牛顿法\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#牛顿法与拟牛顿法\"><span>牛顿法与拟牛顿法</span></a></h4>\n<ul>\n<li><strong>原理</strong>：利用二阶导数（Hessian矩阵）加速收敛</li>\n<li><strong>优点</strong>：收敛速度快</li>\n<li><strong>缺点</strong>：计算和存储成本高，适合小规模问题</li>\n</ul>\n<h3 id=\"_3-正则化与泛化\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_3-正则化与泛化\"><span>3. 正则化与泛化</span></a></h3>\n<ul>\n<li><strong>L1正则化</strong>：<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>λ</mi><mi mathvariant=\"normal\">∥</mi><mi mathvariant=\"bold\">w</mi><msub><mi mathvariant=\"normal\">∥</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">\\lambda \\|\\mathbf{w}\\|_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">λ</span><span class=\"mord\">∥</span><span class=\"mord mathbf\" style=\"margin-right:0.01597em;\">w</span><span class=\"mord\"><span class=\"mord\">∥</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>，促使参数稀疏，特征选择</li>\n<li><strong>L2正则化</strong>：<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>λ</mi><mi mathvariant=\"normal\">∥</mi><mi mathvariant=\"bold\">w</mi><msubsup><mi mathvariant=\"normal\">∥</mi><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding=\"application/x-tex\">\\lambda \\|\\mathbf{w}\\|_2^2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.0641em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">λ</span><span class=\"mord\">∥</span><span class=\"mord mathbf\" style=\"margin-right:0.01597em;\">w</span><span class=\"mord\"><span class=\"mord\">∥</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141em;\"><span style=\"top:-2.4519em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2481em;\"><span></span></span></span></span></span></span></span></span></span>，防止过拟合，参数收缩</li>\n<li><strong>早停法</strong>：在验证集性能不再提升时提前终止训练，防止过拟合</li>\n</ul>\n<h3 id=\"_4-凸优化与非凸优化\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_4-凸优化与非凸优化\"><span>4. 凸优化与非凸优化</span></a></h3>\n<ul>\n<li><strong>凸优化</strong>：目标函数和约束都是凸的，只有一个全局最优解</li>\n<li><strong>非凸优化</strong>：可能有多个局部最优，深度学习/推荐系统常见</li>\n<li><strong>常用技巧</strong>：多次随机初始化、批量归一化、学习率衰减</li>\n</ul>\n<h2 id=\"🛠️-在搜广推中的应用\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#🛠️-在搜广推中的应用\"><span>🛠️ 在搜广推中的应用</span></a></h2>\n<h3 id=\"搜索排序模型训练\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#搜索排序模型训练\"><span>搜索排序模型训练</span></a></h3>\n<ul>\n<li>RankNet、LambdaMART等排序模型本质上都是优化损失函数</li>\n<li>损失函数设计直接影响排序效果</li>\n</ul>\n<h3 id=\"推荐系统embedding训练\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#推荐系统embedding训练\"><span>推荐系统Embedding训练</span></a></h3>\n<ul>\n<li>矩阵分解、Word2Vec、DeepFM等模型都依赖高效优化算法</li>\n<li>大规模分布式SGD、异步更新、负采样等工程技巧</li>\n</ul>\n<h3 id=\"广告竞价与预算分配\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#广告竞价与预算分配\"><span>广告竞价与预算分配</span></a></h3>\n<ul>\n<li>预算约束下的最大化ROI问题</li>\n<li>常用方法：线性规划、动态规划、贪心算法</li>\n</ul>\n<h3 id=\"超参数调优\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#超参数调优\"><span>超参数调优</span></a></h3>\n<ul>\n<li>学习率、正则化系数、批量大小等都需通过优化实验确定</li>\n<li>常用方法：网格搜索、随机搜索、贝叶斯优化</li>\n</ul>\n<h2 id=\"📖-延伸阅读\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📖-延伸阅读\"><span>📖 延伸阅读</span></a></h2>\n<div class=\"hint-container note\">\n<p class=\"hint-container-title\">推荐书籍</p>\n<ol>\n<li><strong>《最优化方法》</strong> - 丁同仁：经典优化教材，理论与算法并重</li>\n<li><strong>《Convex Optimization》</strong> - Boyd &amp; Vandenberghe：凸优化圣经</li>\n<li><strong>《深度学习》</strong> - Ian Goodfellow：深度学习中的优化方法</li>\n</ol>\n</div>\n<blockquote>\n<p>📝 <strong>思考题</strong>：为什么深度学习/推荐系统常常陷入局部最优？你如何设计优化流程来提升大规模模型的训练效率？</p>\n</blockquote>\n<div class=\"hint-container tip\">\n<p class=\"hint-container-title\">🎉 章节小结</p>\n<p>优化方法是让模型&quot;更优&quot;的秘密武器。从梯度下降到牛顿法，从正则化到超参数调优，优化思想贯穿了搜广推系统的每一个环节。掌握优化，就是掌握了让算法持续进化的钥匙。</p>\n</div>\n<blockquote>\n<p><strong>优化就像修炼内功——看不见却决定了你能走多远。</strong></p>\n</blockquote>\n</template>","contentStripped":"<blockquote>\n<p>💡 <strong>高斯说</strong>：没有优化，机器学习只是&quot;调参游戏&quot;。在搜广推领域，优化方法是模型训练、排序、广告竞价的灵魂。</p>\n</blockquote>\n<h2 id=\"🎯-为什么要学优化\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#🎯-为什么要学优化\"><span>🎯 为什么要学优化？</span></a></h2>\n<ul>\n<li>搜索排序模型如何找到最优参数？</li>\n<li>推荐系统如何高效训练大规模Embedding？</li>\n<li>广告竞价如何在预算约束下最大化ROI？</li>\n</ul>\n<p>所有这些问题的本质，都是&quot;在约束下寻找最优解&quot;。优化方法就是解决这些问题的&quot;内功心法&quot;。</p>\n<h2 id=\"📚-核心概念速成\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📚-核心概念速成\"><span>📚 核心概念速成</span></a></h2>\n<h3 id=\"_1-优化问题的基本形式\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_1-优化问题的基本形式\"><span>1. 优化问题的基本形式</span></a></h3>\n<ul>\n<li><strong>目标函数</strong>：<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>f</mi><mo stretchy=\"false\">(</mo><mi mathvariant=\"bold\">x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">f(\\mathbf{x})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord mathbf\">x</span><span class=\"mclose\">)</span></span></span></span>，希望最小化或最大化</li>\n<li><strong>约束条件</strong>：<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>g</mi><mi>i</mi></msub><mo stretchy=\"false\">(</mo><mi mathvariant=\"bold\">x</mi><mo stretchy=\"false\">)</mo><mo>≤</mo><mn>0</mn><mo separator=\"true\">,</mo><msub><mi>h</mi><mi>j</mi></msub><mo stretchy=\"false\">(</mo><mi mathvariant=\"bold\">x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">g_i(\\mathbf{x}) \\leq 0, h_j(\\mathbf{x}) = 0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathbf\">x</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">≤</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0361em;vertical-align:-0.2861em;\"></span><span class=\"mord\">0</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">h</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathbf\">x</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">0</span></span></span></span></li>\n<li><strong>最优解</strong>：使目标函数取得最优值的<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi mathvariant=\"bold\">x</mi><mo>∗</mo></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{x}^*</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6887em;\"></span><span class=\"mord\"><span class=\"mord mathbf\">x</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6887em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mbin mtight\">∗</span></span></span></span></span></span></span></span></span></span></span></li>\n</ul>\n<h3 id=\"_2-常见优化算法\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_2-常见优化算法\"><span>2. 常见优化算法</span></a></h3>\n<h4 id=\"梯度下降法-gradient-descent\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#梯度下降法-gradient-descent\"><span>梯度下降法（Gradient Descent）</span></a></h4>\n<ul>\n<li><strong>原理</strong>：每次沿目标函数的负梯度方向更新参数</li>\n<li><strong>公式</strong>：<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi mathvariant=\"bold\">x</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi mathvariant=\"bold\">x</mi><mi>t</mi></msub><mo>−</mo><mi>η</mi><mi mathvariant=\"normal\">∇</mi><mi>f</mi><mo stretchy=\"false\">(</mo><msub><mi mathvariant=\"bold\">x</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\mathbf{x}_{t+1} = \\mathbf{x}_t - \\eta \\nabla f(\\mathbf{x}_t)\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6528em;vertical-align:-0.2083em;\"></span><span class=\"mord\"><span class=\"mord mathbf\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">t</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathbf\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">η</span><span class=\"mord\">∇</span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathbf\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></span></p>\n其中<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>η</mi></mrow><annotation encoding=\"application/x-tex\">\\eta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">η</span></span></span></span>为学习率</li>\n<li><strong>应用</strong>：线性回归、逻辑回归、神经网络训练</li>\n</ul>\n<h4 id=\"随机梯度下降-sgd\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#随机梯度下降-sgd\"><span>随机梯度下降（SGD）</span></a></h4>\n<ul>\n<li><strong>原理</strong>：每次只用一个样本或小批量样本估算梯度，提升效率</li>\n<li><strong>优点</strong>：适合大规模数据，易于并行</li>\n<li><strong>缺点</strong>：收敛波动大，需要调参</li>\n</ul>\n<h4 id=\"动量法-momentum\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#动量法-momentum\"><span>动量法（Momentum）</span></a></h4>\n<ul>\n<li><strong>原理</strong>：引入&quot;惯性&quot;，加速收敛，减少震荡</li>\n<li><strong>公式</strong>：<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi mathvariant=\"bold\">v</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>γ</mi><msub><mi mathvariant=\"bold\">v</mi><mi>t</mi></msub><mo>+</mo><mi>η</mi><mi mathvariant=\"normal\">∇</mi><mi>f</mi><mo stretchy=\"false\">(</mo><msub><mi mathvariant=\"bold\">x</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo><mspace linebreak=\"newline\"></mspace><msub><mi mathvariant=\"bold\">x</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi mathvariant=\"bold\">x</mi><mi>t</mi></msub><mo>−</mo><msub><mi mathvariant=\"bold\">v</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">\\mathbf{v}_{t+1} = \\gamma \\mathbf{v}_t + \\eta \\nabla f(\\mathbf{x}_t) \\\\\n\\mathbf{x}_{t+1} = \\mathbf{x}_t - \\mathbf{v}_{t+1}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6528em;vertical-align:-0.2083em;\"></span><span class=\"mord\"><span class=\"mord mathbf\" style=\"margin-right:0.01597em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.016em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">t</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7778em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05556em;\">γ</span><span class=\"mord\"><span class=\"mord mathbf\" style=\"margin-right:0.01597em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:-0.016em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">η</span><span class=\"mord\">∇</span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathbf\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span><span class=\"mspace newline\"></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6528em;vertical-align:-0.2083em;\"></span><span class=\"mord\"><span class=\"mord mathbf\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">t</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathbf\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6528em;vertical-align:-0.2083em;\"></span><span class=\"mord\"><span class=\"mord mathbf\" style=\"margin-right:0.01597em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.016em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">t</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span></span></span></span></span></p>\n其中<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>γ</mi></mrow><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05556em;\">γ</span></span></span></span>为动量系数</li>\n</ul>\n<h4 id=\"adam优化器\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#adam优化器\"><span>Adam优化器</span></a></h4>\n<ul>\n<li><strong>原理</strong>：自适应调整每个参数的学习率，结合动量和RMSProp思想</li>\n<li><strong>应用</strong>：深度学习模型训练的主流选择</li>\n</ul>\n<h4 id=\"牛顿法与拟牛顿法\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#牛顿法与拟牛顿法\"><span>牛顿法与拟牛顿法</span></a></h4>\n<ul>\n<li><strong>原理</strong>：利用二阶导数（Hessian矩阵）加速收敛</li>\n<li><strong>优点</strong>：收敛速度快</li>\n<li><strong>缺点</strong>：计算和存储成本高，适合小规模问题</li>\n</ul>\n<h3 id=\"_3-正则化与泛化\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_3-正则化与泛化\"><span>3. 正则化与泛化</span></a></h3>\n<ul>\n<li><strong>L1正则化</strong>：<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>λ</mi><mi mathvariant=\"normal\">∥</mi><mi mathvariant=\"bold\">w</mi><msub><mi mathvariant=\"normal\">∥</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">\\lambda \\|\\mathbf{w}\\|_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">λ</span><span class=\"mord\">∥</span><span class=\"mord mathbf\" style=\"margin-right:0.01597em;\">w</span><span class=\"mord\"><span class=\"mord\">∥</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>，促使参数稀疏，特征选择</li>\n<li><strong>L2正则化</strong>：<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>λ</mi><mi mathvariant=\"normal\">∥</mi><mi mathvariant=\"bold\">w</mi><msubsup><mi mathvariant=\"normal\">∥</mi><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding=\"application/x-tex\">\\lambda \\|\\mathbf{w}\\|_2^2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.0641em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">λ</span><span class=\"mord\">∥</span><span class=\"mord mathbf\" style=\"margin-right:0.01597em;\">w</span><span class=\"mord\"><span class=\"mord\">∥</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141em;\"><span style=\"top:-2.4519em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2481em;\"><span></span></span></span></span></span></span></span></span></span>，防止过拟合，参数收缩</li>\n<li><strong>早停法</strong>：在验证集性能不再提升时提前终止训练，防止过拟合</li>\n</ul>\n<h3 id=\"_4-凸优化与非凸优化\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#_4-凸优化与非凸优化\"><span>4. 凸优化与非凸优化</span></a></h3>\n<ul>\n<li><strong>凸优化</strong>：目标函数和约束都是凸的，只有一个全局最优解</li>\n<li><strong>非凸优化</strong>：可能有多个局部最优，深度学习/推荐系统常见</li>\n<li><strong>常用技巧</strong>：多次随机初始化、批量归一化、学习率衰减</li>\n</ul>\n<h2 id=\"🛠️-在搜广推中的应用\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#🛠️-在搜广推中的应用\"><span>🛠️ 在搜广推中的应用</span></a></h2>\n<h3 id=\"搜索排序模型训练\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#搜索排序模型训练\"><span>搜索排序模型训练</span></a></h3>\n<ul>\n<li>RankNet、LambdaMART等排序模型本质上都是优化损失函数</li>\n<li>损失函数设计直接影响排序效果</li>\n</ul>\n<h3 id=\"推荐系统embedding训练\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#推荐系统embedding训练\"><span>推荐系统Embedding训练</span></a></h3>\n<ul>\n<li>矩阵分解、Word2Vec、DeepFM等模型都依赖高效优化算法</li>\n<li>大规模分布式SGD、异步更新、负采样等工程技巧</li>\n</ul>\n<h3 id=\"广告竞价与预算分配\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#广告竞价与预算分配\"><span>广告竞价与预算分配</span></a></h3>\n<ul>\n<li>预算约束下的最大化ROI问题</li>\n<li>常用方法：线性规划、动态规划、贪心算法</li>\n</ul>\n<h3 id=\"超参数调优\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#超参数调优\"><span>超参数调优</span></a></h3>\n<ul>\n<li>学习率、正则化系数、批量大小等都需通过优化实验确定</li>\n<li>常用方法：网格搜索、随机搜索、贝叶斯优化</li>\n</ul>\n<h2 id=\"📖-延伸阅读\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📖-延伸阅读\"><span>📖 延伸阅读</span></a></h2>\n<div class=\"hint-container note\">\n<p class=\"hint-container-title\">推荐书籍</p>\n<ol>\n<li><strong>《最优化方法》</strong> - 丁同仁：经典优化教材，理论与算法并重</li>\n<li><strong>《Convex Optimization》</strong> - Boyd &amp; Vandenberghe：凸优化圣经</li>\n<li><strong>《深度学习》</strong> - Ian Goodfellow：深度学习中的优化方法</li>\n</ol>\n</div>\n<blockquote>\n<p>📝 <strong>思考题</strong>：为什么深度学习/推荐系统常常陷入局部最优？你如何设计优化流程来提升大规模模型的训练效率？</p>\n</blockquote>\n<div class=\"hint-container tip\">\n<p class=\"hint-container-title\">🎉 章节小结</p>\n<p>优化方法是让模型&quot;更优&quot;的秘密武器。从梯度下降到牛顿法，从正则化到超参数调优，优化思想贯穿了搜广推系统的每一个环节。掌握优化，就是掌握了让算法持续进化的钥匙。</p>\n</div>\n<blockquote>\n<p><strong>优化就像修炼内功——看不见却决定了你能走多远。</strong></p>\n</blockquote>\n","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"> 💡 **高斯说**：没有优化，机器学习只是\"调参游戏\"。在搜广推领域，优化方法是模型训练、排序、广告竞价的灵魂。\n\n## 🎯 为什么要学优化？\n\n- 搜索排序模型如何找到最优参数？\n- 推荐系统如何高效训练大规模Embedding？\n- 广告竞价如何在预算约束下最大化ROI？\n\n所有这些问题的本质，都是\"在约束下寻找最优解\"。优化方法就是解决这些问题的\"内功心法\"。\n\n## 📚 核心概念速成\n\n### 1. 优化问题的基本形式\n\n- **目标函数**：$f(\\mathbf{x})$，希望最小化或最大化\n- **约束条件**：$g_i(\\mathbf{x}) \\leq 0, h_j(\\mathbf{x}) = 0$\n- **最优解**：使目标函数取得最优值的$\\mathbf{x}^*$\n\n### 2. 常见优化算法\n\n#### 梯度下降法（Gradient Descent）\n- **原理**：每次沿目标函数的负梯度方向更新参数\n- **公式**：\n  $$\n  \\mathbf{x}_{t+1} = \\mathbf{x}_t - \\eta \\nabla f(\\mathbf{x}_t)\n  $$\n  其中$\\eta$为学习率\n- **应用**：线性回归、逻辑回归、神经网络训练\n\n#### 随机梯度下降（SGD）\n- **原理**：每次只用一个样本或小批量样本估算梯度，提升效率\n- **优点**：适合大规模数据，易于并行\n- **缺点**：收敛波动大，需要调参\n\n#### 动量法（Momentum）\n- **原理**：引入\"惯性\"，加速收敛，减少震荡\n- **公式**：\n  $$\n  \\mathbf{v}_{t+1} = \\gamma \\mathbf{v}_t + \\eta \\nabla f(\\mathbf{x}_t) \\\\\n  \\mathbf{x}_{t+1} = \\mathbf{x}_t - \\mathbf{v}_{t+1}\n  $$\n  其中$\\gamma$为动量系数\n\n#### Adam优化器\n- **原理**：自适应调整每个参数的学习率，结合动量和RMSProp思想\n- **应用**：深度学习模型训练的主流选择\n\n#### 牛顿法与拟牛顿法\n- **原理**：利用二阶导数（Hessian矩阵）加速收敛\n- **优点**：收敛速度快\n- **缺点**：计算和存储成本高，适合小规模问题\n\n### 3. 正则化与泛化\n\n- **L1正则化**：$\\lambda \\|\\mathbf{w}\\|_1$，促使参数稀疏，特征选择\n- **L2正则化**：$\\lambda \\|\\mathbf{w}\\|_2^2$，防止过拟合，参数收缩\n- **早停法**：在验证集性能不再提升时提前终止训练，防止过拟合\n\n### 4. 凸优化与非凸优化\n\n- **凸优化**：目标函数和约束都是凸的，只有一个全局最优解\n- **非凸优化**：可能有多个局部最优，深度学习/推荐系统常见\n- **常用技巧**：多次随机初始化、批量归一化、学习率衰减\n\n## 🛠️ 在搜广推中的应用\n\n### 搜索排序模型训练\n- RankNet、LambdaMART等排序模型本质上都是优化损失函数\n- 损失函数设计直接影响排序效果\n\n### 推荐系统Embedding训练\n- 矩阵分解、Word2Vec、DeepFM等模型都依赖高效优化算法\n- 大规模分布式SGD、异步更新、负采样等工程技巧\n\n### 广告竞价与预算分配\n- 预算约束下的最大化ROI问题\n- 常用方法：线性规划、动态规划、贪心算法\n\n### 超参数调优\n- 学习率、正则化系数、批量大小等都需通过优化实验确定\n- 常用方法：网格搜索、随机搜索、贝叶斯优化\n\n## 📖 延伸阅读\n\n::: note 推荐书籍\n1. **《最优化方法》** - 丁同仁：经典优化教材，理论与算法并重\n2. **《Convex Optimization》** - Boyd & Vandenberghe：凸优化圣经\n3. **《深度学习》** - Ian Goodfellow：深度学习中的优化方法\n:::\n\n> 📝 **思考题**：为什么深度学习/推荐系统常常陷入局部最优？你如何设计优化流程来提升大规模模型的训练效率？\n\n::: tip 🎉 章节小结\n优化方法是让模型\"更优\"的秘密武器。从梯度下降到牛顿法，从正则化到超参数调优，优化思想贯穿了搜广推系统的每一个环节。掌握优化，就是掌握了让算法持续进化的钥匙。\n:::\n\n> **优化就像修炼内功——看不见却决定了你能走多远。**","excerpt":"","includedFiles":[],"tasklistId":0,"title":"","headers":[{"level":2,"title":"🎯 为什么要学优化？","slug":"🎯-为什么要学优化","link":"#🎯-为什么要学优化","children":[]},{"level":2,"title":"📚 核心概念速成","slug":"📚-核心概念速成","link":"#📚-核心概念速成","children":[{"level":3,"title":"1. 优化问题的基本形式","slug":"_1-优化问题的基本形式","link":"#_1-优化问题的基本形式","children":[]},{"level":3,"title":"2. 常见优化算法","slug":"_2-常见优化算法","link":"#_2-常见优化算法","children":[]},{"level":3,"title":"3. 正则化与泛化","slug":"_3-正则化与泛化","link":"#_3-正则化与泛化","children":[]},{"level":3,"title":"4. 凸优化与非凸优化","slug":"_4-凸优化与非凸优化","link":"#_4-凸优化与非凸优化","children":[]}]},{"level":2,"title":"🛠️ 在搜广推中的应用","slug":"🛠️-在搜广推中的应用","link":"#🛠️-在搜广推中的应用","children":[{"level":3,"title":"搜索排序模型训练","slug":"搜索排序模型训练","link":"#搜索排序模型训练","children":[]},{"level":3,"title":"推荐系统Embedding训练","slug":"推荐系统embedding训练","link":"#推荐系统embedding训练","children":[]},{"level":3,"title":"广告竞价与预算分配","slug":"广告竞价与预算分配","link":"#广告竞价与预算分配","children":[]},{"level":3,"title":"超参数调优","slug":"超参数调优","link":"#超参数调优","children":[]}]},{"level":2,"title":"📖 延伸阅读","slug":"📖-延伸阅读","link":"#📖-延伸阅读","children":[]}]}}
