{"content":"<p>在推荐系统的远古时代，计算机眼里的世界是离散且无情的。它如何识别一个商品？答案是：<strong>One-Hot编码</strong>。</p>\n<p>想象一个拥有100万商品的大型电商平台：</p>\n<ul>\n<li><strong>iPhone 14</strong> 的编码可能是 <code v-pre>[0, 0, ..., 1, ..., 0, 0]</code> (在第8888个位置是1)</li>\n<li><strong>华为 Mate 60</strong> 的编码可能是 <code v-pre>[0, 1, ..., 0, ..., 0, 0]</code> (在第12345个位置是1)</li>\n<li><strong>一本《三体》</strong> 的编码可能是 <code v-pre>[0, 0, ..., 0, ..., 1, 0]</code> (在第54321个位置是1)</li>\n</ul>\n<p>这套系统有两个致命缺陷：</p>\n<ol>\n<li><strong>语义的真空</strong>：从数学上看，<code v-pre>iPhone 14</code> 和 <code v-pre>华为 Mate 60</code> 的向量是完全正交的，它们的距离和 <code v-pre>iPhone 14</code> 与 <code v-pre>三体</code> 是一样远的。计算机完全是个&quot;脸盲&quot;，无法理解商品之间的任何关联。</li>\n<li><strong>维度的诅咒</strong>：向量巨大且极其稀疏，这对于存储和计算都是一场灾难。</li>\n</ol>\n<p>为了解决语义问题，人们尝试了<strong>人工打标签</strong>，就像给每个商品都办一张&quot;身份证&quot;（标签：电子产品、手机、Apple...）。但这太耗费人力，且标签的粒度和质量都难以控制。</p>\n<p>世界需要一场革命，一场能让计算机自动理解万物关联的革命。</p>\n<h2 id=\"创世纪-word2vec-与-上下文-的神谕-✨\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#创世纪-word2vec-与-上下文-的神谕-✨\"><span>创世纪：Word2Vec 与&quot;上下文&quot;的神谕 ✨</span></a></h2>\n<p>革命的曙光来自看似毫不相关的NLP（自然语言处理）领域。2013年，Google的Word2Vec带来了神谕：</p>\n<blockquote>\n<p><strong>一个词的意义，由其周围的词来定义 (Context is King)。</strong></p>\n</blockquote>\n<p>这个思想简单却颠覆认知。比如，在海量文本中：</p>\n<ul>\n<li>&quot;国王&quot; 常常和 &quot;王后&quot;、&quot;权力&quot;、&quot;国家&quot; 一起出现。</li>\n<li>&quot;皇帝&quot; 也常常和 &quot;皇后&quot;、&quot;权力&quot;、&quot;国家&quot; 一起出现。</li>\n</ul>\n<p>那么，模型就能自动推断出：&quot;国王&quot;和&quot;皇帝&quot;的语义是相近的。</p>\n<p>Word2Vec做的，就是把每个词语，从高维稀疏的One-Hot向量，<strong>&quot;降维打击&quot;</strong> 成一个低维（比如300维）的稠密向量——这，就是 <strong>Embedding</strong>。</p>\n<h2 id=\"语义宇宙-embedding空间的奇妙法则-🌌\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#语义宇宙-embedding空间的奇妙法则-🌌\"><span>&quot;语义宇宙&quot;：Embedding空间的奇妙法则 🌌</span></a></h2>\n<p>这个由Embedding向量构成的低维空间，如同一个&quot;语义宇宙&quot;，遵循着奇妙的物理法则：</p>\n<h4 id=\"法则一-距离代表相似性\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#法则一-距离代表相似性\"><span>法则一：距离代表相似性</span></a></h4>\n<p>在这个宇宙里，语义相近的物体，它们的&quot;空间坐标&quot;（Embedding向量）也相互靠近。</p>\n<ul>\n<li><code v-pre>distance(国王, 皇帝)</code> → 很近</li>\n<li><code v-pre>distance(国王, 香蕉)</code> → 很远</li>\n</ul>\n<h4 id=\"法则二-方向蕴含关系\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#法则二-方向蕴含关系\"><span>法则二：方向蕴含关系</span></a></h4>\n<p>向量的方向和差异，竟然蕴含了抽象的语义关系。最经典的例子就是：</p>\n<blockquote>\n<p><code v-pre>vector(国王) - vector(男性) + vector(女性) ≈ vector(王后)</code></p>\n</blockquote>\n<p>这意味着，从&quot;国王&quot;的位置，沿着&quot;从男性指向女性&quot;的方向移动，你就能到达&quot;王后&quot;的附近！这个简单的数学运算，打开了语义计算的大门。</p>\n<h2 id=\"从文本到推荐-item2vec-的-思想迁跃-🚀\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#从文本到推荐-item2vec-的-思想迁跃-🚀\"><span>从文本到推荐：Item2Vec 的&quot;思想迁跃&quot; 🚀</span></a></h2>\n<p>推荐系统的大师们敏锐地意识到，这个思想可以完美地&quot;迁跃&quot;到电商领域：</p>\n<blockquote>\n<p><strong>一个商品的意义，由和它一起被浏览/购买的商品来定义。</strong></p>\n</blockquote>\n<p>于是，Item2Vec诞生了：</p>\n<ul>\n<li><strong>语料库 (Corpus)</strong>：不再是文章句子，而是用户的行为序列（如点击序列、购买序列）。</li>\n<li><strong>单词 (Word)</strong>：变成了商品ID。</li>\n</ul>\n<p>一个用户的购买序列 <code v-pre>[iPhone, AirPods, Apple Watch]</code> 就相当于一个句子。通过学习大量的这类&quot;句子&quot;，模型自动学会了：</p>\n<ul>\n<li><code v-pre>Embedding(iPhone)</code> 和 <code v-pre>Embedding(AirPods)</code> 在空间中应该很近。</li>\n<li><code v-pre>vector(iPhone) - vector(Apple) + vector(小米) ≈ vector(小米手机)</code> 这种品牌和产品的关系。</li>\n</ul>\n<p><strong>Embedding，第一次赋予了每个商品独一无二、可计算的&quot;灵魂&quot;。</strong></p>\n<h2 id=\"现代应用-embedding-如何驱动推荐系统-🎯\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#现代应用-embedding-如何驱动推荐系统-🎯\"><span>现代应用：Embedding 如何驱动推荐系统？ 🎯</span></a></h2>\n<p>在现代推荐系统中，Embedding早已成为水和电一样的基础设施，其应用贯穿始终。</p>\n<h3 id=\"阶段一-召回-——-在亿万商品中-物以类聚\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#阶段一-召回-——-在亿万商品中-物以类聚\"><span>阶段一：召回 —— 在亿万商品中&quot;物以类聚&quot;</span></a></h3>\n<p>当一个用户来访时，我们如何从亿万商品中，快速找到他可能感兴趣的几千个候选者？答案就是利用Embedding进行高效的相似性搜索。</p>\n<p>这通常通过<strong>双塔模型 (Two-Tower Model)</strong> 实现：</p>\n<ul>\n<li><strong>用户塔 (User Tower)</strong>：将用户的ID、历史行为序列等信息，通过一个网络（如DNN、RNN）计算出该用户的实时<code v-pre>User Embedding</code>。</li>\n<li><strong>物品塔 (Item Tower)</strong>：离线计算好所有商品的<code v-pre>Item Embedding</code>。</li>\n</ul>\n<p>推荐过程就变成了：拿着实时的<code v-pre>User Embedding</code>，去庞大的<code v-pre>Item Embedding</code>宇宙中，通过<strong>近似最近邻搜索 (ANN)</strong> 技术（如Faiss、HNSW），光速般地找出与用户向量最相似的Top-K个商品。</p>\n<h3 id=\"阶段二-排序-——-让embedding成为模型的-眼睛\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#阶段二-排序-——-让embedding成为模型的-眼睛\"><span>阶段二：排序 —— 让Embedding成为模型的&quot;眼睛&quot;</span></a></h3>\n<p>召回阶段选出的几千个商品，还需要一个更精细的模型（即<strong>精排模型</strong>，如DeepFM）来精准预估点击率（CTR），决定最终的呈现顺序。</p>\n<p>在这里，Embedding扮演了另一个关键角色——<strong>作为特征输入</strong>。</p>\n<ul>\n<li><strong>过去</strong>：精排模型看到的输入是 <code v-pre>item_id=8888</code> 这种无意义的ID。</li>\n<li><strong>现在</strong>：模型看到的输入是<code v-pre>item_embedding=[0.12, -0.45, ..., 0.88]</code> 这样一个充满语义的向量。</li>\n</ul>\n<p>这等于给模型装上了一双能理解商品内在含义的&quot;眼睛&quot;。它不再是处理一堆ID，而是在处理具有丰富内涵的语义信息，从而能学习到更复杂的交叉模式，做出更精准的判断。</p>\n<h3 id=\"拓展应用-解决-冷启动-等疑难杂症\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#拓展应用-解决-冷启动-等疑难杂症\"><span>拓展应用：解决&quot;冷启动&quot;等疑难杂症</span></a></h3>\n<p>Embedding的魔力不止于此，它还能优雅地解决很多业界难题：</p>\n<ul>\n<li><strong>冷启动 (Cold Start)</strong>：一个新商品没有任何行为，如何为它生成Embedding？我们可以利用它的<strong>内容信息</strong>，比如将其文字描述（通过BERT）和图片（通过ResNet）转换成Embedding，给它一个合理的初始&quot;宇宙坐标&quot;。</li>\n<li><strong>跨领域推荐</strong>：可以将新闻的Embedding和商品的Embedding映射到同一个语义空间，从而实现从新闻偏好到商品推荐的跨越。</li>\n</ul>\n<h2 id=\"📖-延伸阅读\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📖-延伸阅读\"><span>📖 <strong>延伸阅读</strong></span></a></h2>\n<ol>\n<li><a href=\"https://arxiv.org/abs/1301.3781\" target=\"_blank\" rel=\"noopener noreferrer\">Efficient Estimation of Word Representations in Vector Space</a> - Word2Vec 原始论文，一切开始的地方。</li>\n<li><a href=\"https://arxiv.org/abs/1603.04259\" target=\"_blank\" rel=\"noopener noreferrer\">Item2Vec: Neural Item Embedding for Collaborative Filtering</a> - 将Embedding思想迁移到推荐系统的经典之作。</li>\n<li><a href=\"https://arxiv.org/abs/1803.02349\" target=\"_blank\" rel=\"noopener noreferrer\">Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba</a> - 阿里巴巴关于构建工业级大规模商品Embedding的雄文。</li>\n<li><a href=\"https://www.kdd.org/kdd2018/accepted-papers/view/real-time-personalization-using-embeddings-for-search-ranking-at-airbnb\" target=\"_blank\" rel=\"noopener noreferrer\">Real-time Personalization using Embeddings for Search Ranking at Airbnb</a> - Airbnb在实时搜索推荐中应用Embedding的经典案例。</li>\n</ol>\n<blockquote>\n<p>🧠 <strong>思考题</strong></p>\n<ol>\n<li>在双塔模型中，为什么用户塔和物品塔的Embedding通常需要保持维度一致？</li>\n<li>除了用户行为序列，你还能想到哪些数据可以用来为商品生成更有意义的Embedding？（例如，商品的文本描述、图片、价格、品牌等）</li>\n<li>你认为<code v-pre>king - man + woman = queen</code>这个类比，在电商场景下的<code v-pre>iPhone - Apple + Huawei ≈ Pura70</code>，其背后的成功本质是什么？</li>\n</ol>\n</blockquote>\n<div class=\"hint-container tip\">\n<p class=\"hint-container-title\">🎉 章节小结</p>\n<p>Embedding是深度学习时代推荐系统的基石，它将离散的ID世界转化为连续的语义空间。从Word2Vec的&quot;上下文定义语义&quot;到Item2Vec的&quot;共现定义关联&quot;，Embedding让机器第一次拥有了理解事物内在联系的能力。无论是召回阶段的相似性搜索，还是排序阶段的特征输入，Embedding都扮演着不可替代的角色。它不仅解决了稀疏性和冷启动问题，更重要的是为推荐系统构建了一个可计算的&quot;语义宇宙&quot;。</p>\n</div>\n<hr>\n<blockquote>\n<p>&quot;You shall know a word by the company it keeps.&quot; —— John Rupert Firth<br>\n在推荐系统的世界里，你也将通过一个物品的&quot;伙伴&quot;来了解它的本质。</p>\n</blockquote>\n","env":{"base":"/search-rec-ads-cosmos-explorer/","filePath":"D:/softwore/user/git/work_code/WeBotDoc/docs/zh/3.第三章：推荐算法--比你更懂你的贴心小棉袄/2.深度学习文艺复兴/2.embedding.md","filePathRelative":"zh/3.第三章：推荐算法--比你更懂你的贴心小棉袄/2.深度学习文艺复兴/2.embedding.md","frontmatter":{"title":"Embedding：从文本到推荐的语义桥梁","createTime":"2025/06/12 10:00:00"},"sfcBlocks":{"template":{"type":"template","content":"<template><p>在推荐系统的远古时代，计算机眼里的世界是离散且无情的。它如何识别一个商品？答案是：<strong>One-Hot编码</strong>。</p>\n<p>想象一个拥有100万商品的大型电商平台：</p>\n<ul>\n<li><strong>iPhone 14</strong> 的编码可能是 <code v-pre>[0, 0, ..., 1, ..., 0, 0]</code> (在第8888个位置是1)</li>\n<li><strong>华为 Mate 60</strong> 的编码可能是 <code v-pre>[0, 1, ..., 0, ..., 0, 0]</code> (在第12345个位置是1)</li>\n<li><strong>一本《三体》</strong> 的编码可能是 <code v-pre>[0, 0, ..., 0, ..., 1, 0]</code> (在第54321个位置是1)</li>\n</ul>\n<p>这套系统有两个致命缺陷：</p>\n<ol>\n<li><strong>语义的真空</strong>：从数学上看，<code v-pre>iPhone 14</code> 和 <code v-pre>华为 Mate 60</code> 的向量是完全正交的，它们的距离和 <code v-pre>iPhone 14</code> 与 <code v-pre>三体</code> 是一样远的。计算机完全是个&quot;脸盲&quot;，无法理解商品之间的任何关联。</li>\n<li><strong>维度的诅咒</strong>：向量巨大且极其稀疏，这对于存储和计算都是一场灾难。</li>\n</ol>\n<p>为了解决语义问题，人们尝试了<strong>人工打标签</strong>，就像给每个商品都办一张&quot;身份证&quot;（标签：电子产品、手机、Apple...）。但这太耗费人力，且标签的粒度和质量都难以控制。</p>\n<p>世界需要一场革命，一场能让计算机自动理解万物关联的革命。</p>\n<h2 id=\"创世纪-word2vec-与-上下文-的神谕-✨\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#创世纪-word2vec-与-上下文-的神谕-✨\"><span>创世纪：Word2Vec 与&quot;上下文&quot;的神谕 ✨</span></a></h2>\n<p>革命的曙光来自看似毫不相关的NLP（自然语言处理）领域。2013年，Google的Word2Vec带来了神谕：</p>\n<blockquote>\n<p><strong>一个词的意义，由其周围的词来定义 (Context is King)。</strong></p>\n</blockquote>\n<p>这个思想简单却颠覆认知。比如，在海量文本中：</p>\n<ul>\n<li>&quot;国王&quot; 常常和 &quot;王后&quot;、&quot;权力&quot;、&quot;国家&quot; 一起出现。</li>\n<li>&quot;皇帝&quot; 也常常和 &quot;皇后&quot;、&quot;权力&quot;、&quot;国家&quot; 一起出现。</li>\n</ul>\n<p>那么，模型就能自动推断出：&quot;国王&quot;和&quot;皇帝&quot;的语义是相近的。</p>\n<p>Word2Vec做的，就是把每个词语，从高维稀疏的One-Hot向量，<strong>&quot;降维打击&quot;</strong> 成一个低维（比如300维）的稠密向量——这，就是 <strong>Embedding</strong>。</p>\n<h2 id=\"语义宇宙-embedding空间的奇妙法则-🌌\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#语义宇宙-embedding空间的奇妙法则-🌌\"><span>&quot;语义宇宙&quot;：Embedding空间的奇妙法则 🌌</span></a></h2>\n<p>这个由Embedding向量构成的低维空间，如同一个&quot;语义宇宙&quot;，遵循着奇妙的物理法则：</p>\n<h4 id=\"法则一-距离代表相似性\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#法则一-距离代表相似性\"><span>法则一：距离代表相似性</span></a></h4>\n<p>在这个宇宙里，语义相近的物体，它们的&quot;空间坐标&quot;（Embedding向量）也相互靠近。</p>\n<ul>\n<li><code v-pre>distance(国王, 皇帝)</code> → 很近</li>\n<li><code v-pre>distance(国王, 香蕉)</code> → 很远</li>\n</ul>\n<h4 id=\"法则二-方向蕴含关系\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#法则二-方向蕴含关系\"><span>法则二：方向蕴含关系</span></a></h4>\n<p>向量的方向和差异，竟然蕴含了抽象的语义关系。最经典的例子就是：</p>\n<blockquote>\n<p><code v-pre>vector(国王) - vector(男性) + vector(女性) ≈ vector(王后)</code></p>\n</blockquote>\n<p>这意味着，从&quot;国王&quot;的位置，沿着&quot;从男性指向女性&quot;的方向移动，你就能到达&quot;王后&quot;的附近！这个简单的数学运算，打开了语义计算的大门。</p>\n<h2 id=\"从文本到推荐-item2vec-的-思想迁跃-🚀\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#从文本到推荐-item2vec-的-思想迁跃-🚀\"><span>从文本到推荐：Item2Vec 的&quot;思想迁跃&quot; 🚀</span></a></h2>\n<p>推荐系统的大师们敏锐地意识到，这个思想可以完美地&quot;迁跃&quot;到电商领域：</p>\n<blockquote>\n<p><strong>一个商品的意义，由和它一起被浏览/购买的商品来定义。</strong></p>\n</blockquote>\n<p>于是，Item2Vec诞生了：</p>\n<ul>\n<li><strong>语料库 (Corpus)</strong>：不再是文章句子，而是用户的行为序列（如点击序列、购买序列）。</li>\n<li><strong>单词 (Word)</strong>：变成了商品ID。</li>\n</ul>\n<p>一个用户的购买序列 <code v-pre>[iPhone, AirPods, Apple Watch]</code> 就相当于一个句子。通过学习大量的这类&quot;句子&quot;，模型自动学会了：</p>\n<ul>\n<li><code v-pre>Embedding(iPhone)</code> 和 <code v-pre>Embedding(AirPods)</code> 在空间中应该很近。</li>\n<li><code v-pre>vector(iPhone) - vector(Apple) + vector(小米) ≈ vector(小米手机)</code> 这种品牌和产品的关系。</li>\n</ul>\n<p><strong>Embedding，第一次赋予了每个商品独一无二、可计算的&quot;灵魂&quot;。</strong></p>\n<h2 id=\"现代应用-embedding-如何驱动推荐系统-🎯\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#现代应用-embedding-如何驱动推荐系统-🎯\"><span>现代应用：Embedding 如何驱动推荐系统？ 🎯</span></a></h2>\n<p>在现代推荐系统中，Embedding早已成为水和电一样的基础设施，其应用贯穿始终。</p>\n<h3 id=\"阶段一-召回-——-在亿万商品中-物以类聚\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#阶段一-召回-——-在亿万商品中-物以类聚\"><span>阶段一：召回 —— 在亿万商品中&quot;物以类聚&quot;</span></a></h3>\n<p>当一个用户来访时，我们如何从亿万商品中，快速找到他可能感兴趣的几千个候选者？答案就是利用Embedding进行高效的相似性搜索。</p>\n<p>这通常通过<strong>双塔模型 (Two-Tower Model)</strong> 实现：</p>\n<ul>\n<li><strong>用户塔 (User Tower)</strong>：将用户的ID、历史行为序列等信息，通过一个网络（如DNN、RNN）计算出该用户的实时<code v-pre>User Embedding</code>。</li>\n<li><strong>物品塔 (Item Tower)</strong>：离线计算好所有商品的<code v-pre>Item Embedding</code>。</li>\n</ul>\n<p>推荐过程就变成了：拿着实时的<code v-pre>User Embedding</code>，去庞大的<code v-pre>Item Embedding</code>宇宙中，通过<strong>近似最近邻搜索 (ANN)</strong> 技术（如Faiss、HNSW），光速般地找出与用户向量最相似的Top-K个商品。</p>\n<h3 id=\"阶段二-排序-——-让embedding成为模型的-眼睛\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#阶段二-排序-——-让embedding成为模型的-眼睛\"><span>阶段二：排序 —— 让Embedding成为模型的&quot;眼睛&quot;</span></a></h3>\n<p>召回阶段选出的几千个商品，还需要一个更精细的模型（即<strong>精排模型</strong>，如DeepFM）来精准预估点击率（CTR），决定最终的呈现顺序。</p>\n<p>在这里，Embedding扮演了另一个关键角色——<strong>作为特征输入</strong>。</p>\n<ul>\n<li><strong>过去</strong>：精排模型看到的输入是 <code v-pre>item_id=8888</code> 这种无意义的ID。</li>\n<li><strong>现在</strong>：模型看到的输入是<code v-pre>item_embedding=[0.12, -0.45, ..., 0.88]</code> 这样一个充满语义的向量。</li>\n</ul>\n<p>这等于给模型装上了一双能理解商品内在含义的&quot;眼睛&quot;。它不再是处理一堆ID，而是在处理具有丰富内涵的语义信息，从而能学习到更复杂的交叉模式，做出更精准的判断。</p>\n<h3 id=\"拓展应用-解决-冷启动-等疑难杂症\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#拓展应用-解决-冷启动-等疑难杂症\"><span>拓展应用：解决&quot;冷启动&quot;等疑难杂症</span></a></h3>\n<p>Embedding的魔力不止于此，它还能优雅地解决很多业界难题：</p>\n<ul>\n<li><strong>冷启动 (Cold Start)</strong>：一个新商品没有任何行为，如何为它生成Embedding？我们可以利用它的<strong>内容信息</strong>，比如将其文字描述（通过BERT）和图片（通过ResNet）转换成Embedding，给它一个合理的初始&quot;宇宙坐标&quot;。</li>\n<li><strong>跨领域推荐</strong>：可以将新闻的Embedding和商品的Embedding映射到同一个语义空间，从而实现从新闻偏好到商品推荐的跨越。</li>\n</ul>\n<h2 id=\"📖-延伸阅读\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📖-延伸阅读\"><span>📖 <strong>延伸阅读</strong></span></a></h2>\n<ol>\n<li><a href=\"https://arxiv.org/abs/1301.3781\" target=\"_blank\" rel=\"noopener noreferrer\">Efficient Estimation of Word Representations in Vector Space</a> - Word2Vec 原始论文，一切开始的地方。</li>\n<li><a href=\"https://arxiv.org/abs/1603.04259\" target=\"_blank\" rel=\"noopener noreferrer\">Item2Vec: Neural Item Embedding for Collaborative Filtering</a> - 将Embedding思想迁移到推荐系统的经典之作。</li>\n<li><a href=\"https://arxiv.org/abs/1803.02349\" target=\"_blank\" rel=\"noopener noreferrer\">Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba</a> - 阿里巴巴关于构建工业级大规模商品Embedding的雄文。</li>\n<li><a href=\"https://www.kdd.org/kdd2018/accepted-papers/view/real-time-personalization-using-embeddings-for-search-ranking-at-airbnb\" target=\"_blank\" rel=\"noopener noreferrer\">Real-time Personalization using Embeddings for Search Ranking at Airbnb</a> - Airbnb在实时搜索推荐中应用Embedding的经典案例。</li>\n</ol>\n<blockquote>\n<p>🧠 <strong>思考题</strong></p>\n<ol>\n<li>在双塔模型中，为什么用户塔和物品塔的Embedding通常需要保持维度一致？</li>\n<li>除了用户行为序列，你还能想到哪些数据可以用来为商品生成更有意义的Embedding？（例如，商品的文本描述、图片、价格、品牌等）</li>\n<li>你认为<code v-pre>king - man + woman = queen</code>这个类比，在电商场景下的<code v-pre>iPhone - Apple + Huawei ≈ Pura70</code>，其背后的成功本质是什么？</li>\n</ol>\n</blockquote>\n<div class=\"hint-container tip\">\n<p class=\"hint-container-title\">🎉 章节小结</p>\n<p>Embedding是深度学习时代推荐系统的基石，它将离散的ID世界转化为连续的语义空间。从Word2Vec的&quot;上下文定义语义&quot;到Item2Vec的&quot;共现定义关联&quot;，Embedding让机器第一次拥有了理解事物内在联系的能力。无论是召回阶段的相似性搜索，还是排序阶段的特征输入，Embedding都扮演着不可替代的角色。它不仅解决了稀疏性和冷启动问题，更重要的是为推荐系统构建了一个可计算的&quot;语义宇宙&quot;。</p>\n</div>\n<hr>\n<blockquote>\n<p>&quot;You shall know a word by the company it keeps.&quot; —— John Rupert Firth<br>\n在推荐系统的世界里，你也将通过一个物品的&quot;伙伴&quot;来了解它的本质。</p>\n</blockquote>\n</template>","contentStripped":"<p>在推荐系统的远古时代，计算机眼里的世界是离散且无情的。它如何识别一个商品？答案是：<strong>One-Hot编码</strong>。</p>\n<p>想象一个拥有100万商品的大型电商平台：</p>\n<ul>\n<li><strong>iPhone 14</strong> 的编码可能是 <code v-pre>[0, 0, ..., 1, ..., 0, 0]</code> (在第8888个位置是1)</li>\n<li><strong>华为 Mate 60</strong> 的编码可能是 <code v-pre>[0, 1, ..., 0, ..., 0, 0]</code> (在第12345个位置是1)</li>\n<li><strong>一本《三体》</strong> 的编码可能是 <code v-pre>[0, 0, ..., 0, ..., 1, 0]</code> (在第54321个位置是1)</li>\n</ul>\n<p>这套系统有两个致命缺陷：</p>\n<ol>\n<li><strong>语义的真空</strong>：从数学上看，<code v-pre>iPhone 14</code> 和 <code v-pre>华为 Mate 60</code> 的向量是完全正交的，它们的距离和 <code v-pre>iPhone 14</code> 与 <code v-pre>三体</code> 是一样远的。计算机完全是个&quot;脸盲&quot;，无法理解商品之间的任何关联。</li>\n<li><strong>维度的诅咒</strong>：向量巨大且极其稀疏，这对于存储和计算都是一场灾难。</li>\n</ol>\n<p>为了解决语义问题，人们尝试了<strong>人工打标签</strong>，就像给每个商品都办一张&quot;身份证&quot;（标签：电子产品、手机、Apple...）。但这太耗费人力，且标签的粒度和质量都难以控制。</p>\n<p>世界需要一场革命，一场能让计算机自动理解万物关联的革命。</p>\n<h2 id=\"创世纪-word2vec-与-上下文-的神谕-✨\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#创世纪-word2vec-与-上下文-的神谕-✨\"><span>创世纪：Word2Vec 与&quot;上下文&quot;的神谕 ✨</span></a></h2>\n<p>革命的曙光来自看似毫不相关的NLP（自然语言处理）领域。2013年，Google的Word2Vec带来了神谕：</p>\n<blockquote>\n<p><strong>一个词的意义，由其周围的词来定义 (Context is King)。</strong></p>\n</blockquote>\n<p>这个思想简单却颠覆认知。比如，在海量文本中：</p>\n<ul>\n<li>&quot;国王&quot; 常常和 &quot;王后&quot;、&quot;权力&quot;、&quot;国家&quot; 一起出现。</li>\n<li>&quot;皇帝&quot; 也常常和 &quot;皇后&quot;、&quot;权力&quot;、&quot;国家&quot; 一起出现。</li>\n</ul>\n<p>那么，模型就能自动推断出：&quot;国王&quot;和&quot;皇帝&quot;的语义是相近的。</p>\n<p>Word2Vec做的，就是把每个词语，从高维稀疏的One-Hot向量，<strong>&quot;降维打击&quot;</strong> 成一个低维（比如300维）的稠密向量——这，就是 <strong>Embedding</strong>。</p>\n<h2 id=\"语义宇宙-embedding空间的奇妙法则-🌌\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#语义宇宙-embedding空间的奇妙法则-🌌\"><span>&quot;语义宇宙&quot;：Embedding空间的奇妙法则 🌌</span></a></h2>\n<p>这个由Embedding向量构成的低维空间，如同一个&quot;语义宇宙&quot;，遵循着奇妙的物理法则：</p>\n<h4 id=\"法则一-距离代表相似性\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#法则一-距离代表相似性\"><span>法则一：距离代表相似性</span></a></h4>\n<p>在这个宇宙里，语义相近的物体，它们的&quot;空间坐标&quot;（Embedding向量）也相互靠近。</p>\n<ul>\n<li><code v-pre>distance(国王, 皇帝)</code> → 很近</li>\n<li><code v-pre>distance(国王, 香蕉)</code> → 很远</li>\n</ul>\n<h4 id=\"法则二-方向蕴含关系\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#法则二-方向蕴含关系\"><span>法则二：方向蕴含关系</span></a></h4>\n<p>向量的方向和差异，竟然蕴含了抽象的语义关系。最经典的例子就是：</p>\n<blockquote>\n<p><code v-pre>vector(国王) - vector(男性) + vector(女性) ≈ vector(王后)</code></p>\n</blockquote>\n<p>这意味着，从&quot;国王&quot;的位置，沿着&quot;从男性指向女性&quot;的方向移动，你就能到达&quot;王后&quot;的附近！这个简单的数学运算，打开了语义计算的大门。</p>\n<h2 id=\"从文本到推荐-item2vec-的-思想迁跃-🚀\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#从文本到推荐-item2vec-的-思想迁跃-🚀\"><span>从文本到推荐：Item2Vec 的&quot;思想迁跃&quot; 🚀</span></a></h2>\n<p>推荐系统的大师们敏锐地意识到，这个思想可以完美地&quot;迁跃&quot;到电商领域：</p>\n<blockquote>\n<p><strong>一个商品的意义，由和它一起被浏览/购买的商品来定义。</strong></p>\n</blockquote>\n<p>于是，Item2Vec诞生了：</p>\n<ul>\n<li><strong>语料库 (Corpus)</strong>：不再是文章句子，而是用户的行为序列（如点击序列、购买序列）。</li>\n<li><strong>单词 (Word)</strong>：变成了商品ID。</li>\n</ul>\n<p>一个用户的购买序列 <code v-pre>[iPhone, AirPods, Apple Watch]</code> 就相当于一个句子。通过学习大量的这类&quot;句子&quot;，模型自动学会了：</p>\n<ul>\n<li><code v-pre>Embedding(iPhone)</code> 和 <code v-pre>Embedding(AirPods)</code> 在空间中应该很近。</li>\n<li><code v-pre>vector(iPhone) - vector(Apple) + vector(小米) ≈ vector(小米手机)</code> 这种品牌和产品的关系。</li>\n</ul>\n<p><strong>Embedding，第一次赋予了每个商品独一无二、可计算的&quot;灵魂&quot;。</strong></p>\n<h2 id=\"现代应用-embedding-如何驱动推荐系统-🎯\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#现代应用-embedding-如何驱动推荐系统-🎯\"><span>现代应用：Embedding 如何驱动推荐系统？ 🎯</span></a></h2>\n<p>在现代推荐系统中，Embedding早已成为水和电一样的基础设施，其应用贯穿始终。</p>\n<h3 id=\"阶段一-召回-——-在亿万商品中-物以类聚\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#阶段一-召回-——-在亿万商品中-物以类聚\"><span>阶段一：召回 —— 在亿万商品中&quot;物以类聚&quot;</span></a></h3>\n<p>当一个用户来访时，我们如何从亿万商品中，快速找到他可能感兴趣的几千个候选者？答案就是利用Embedding进行高效的相似性搜索。</p>\n<p>这通常通过<strong>双塔模型 (Two-Tower Model)</strong> 实现：</p>\n<ul>\n<li><strong>用户塔 (User Tower)</strong>：将用户的ID、历史行为序列等信息，通过一个网络（如DNN、RNN）计算出该用户的实时<code v-pre>User Embedding</code>。</li>\n<li><strong>物品塔 (Item Tower)</strong>：离线计算好所有商品的<code v-pre>Item Embedding</code>。</li>\n</ul>\n<p>推荐过程就变成了：拿着实时的<code v-pre>User Embedding</code>，去庞大的<code v-pre>Item Embedding</code>宇宙中，通过<strong>近似最近邻搜索 (ANN)</strong> 技术（如Faiss、HNSW），光速般地找出与用户向量最相似的Top-K个商品。</p>\n<h3 id=\"阶段二-排序-——-让embedding成为模型的-眼睛\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#阶段二-排序-——-让embedding成为模型的-眼睛\"><span>阶段二：排序 —— 让Embedding成为模型的&quot;眼睛&quot;</span></a></h3>\n<p>召回阶段选出的几千个商品，还需要一个更精细的模型（即<strong>精排模型</strong>，如DeepFM）来精准预估点击率（CTR），决定最终的呈现顺序。</p>\n<p>在这里，Embedding扮演了另一个关键角色——<strong>作为特征输入</strong>。</p>\n<ul>\n<li><strong>过去</strong>：精排模型看到的输入是 <code v-pre>item_id=8888</code> 这种无意义的ID。</li>\n<li><strong>现在</strong>：模型看到的输入是<code v-pre>item_embedding=[0.12, -0.45, ..., 0.88]</code> 这样一个充满语义的向量。</li>\n</ul>\n<p>这等于给模型装上了一双能理解商品内在含义的&quot;眼睛&quot;。它不再是处理一堆ID，而是在处理具有丰富内涵的语义信息，从而能学习到更复杂的交叉模式，做出更精准的判断。</p>\n<h3 id=\"拓展应用-解决-冷启动-等疑难杂症\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#拓展应用-解决-冷启动-等疑难杂症\"><span>拓展应用：解决&quot;冷启动&quot;等疑难杂症</span></a></h3>\n<p>Embedding的魔力不止于此，它还能优雅地解决很多业界难题：</p>\n<ul>\n<li><strong>冷启动 (Cold Start)</strong>：一个新商品没有任何行为，如何为它生成Embedding？我们可以利用它的<strong>内容信息</strong>，比如将其文字描述（通过BERT）和图片（通过ResNet）转换成Embedding，给它一个合理的初始&quot;宇宙坐标&quot;。</li>\n<li><strong>跨领域推荐</strong>：可以将新闻的Embedding和商品的Embedding映射到同一个语义空间，从而实现从新闻偏好到商品推荐的跨越。</li>\n</ul>\n<h2 id=\"📖-延伸阅读\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#📖-延伸阅读\"><span>📖 <strong>延伸阅读</strong></span></a></h2>\n<ol>\n<li><a href=\"https://arxiv.org/abs/1301.3781\" target=\"_blank\" rel=\"noopener noreferrer\">Efficient Estimation of Word Representations in Vector Space</a> - Word2Vec 原始论文，一切开始的地方。</li>\n<li><a href=\"https://arxiv.org/abs/1603.04259\" target=\"_blank\" rel=\"noopener noreferrer\">Item2Vec: Neural Item Embedding for Collaborative Filtering</a> - 将Embedding思想迁移到推荐系统的经典之作。</li>\n<li><a href=\"https://arxiv.org/abs/1803.02349\" target=\"_blank\" rel=\"noopener noreferrer\">Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba</a> - 阿里巴巴关于构建工业级大规模商品Embedding的雄文。</li>\n<li><a href=\"https://www.kdd.org/kdd2018/accepted-papers/view/real-time-personalization-using-embeddings-for-search-ranking-at-airbnb\" target=\"_blank\" rel=\"noopener noreferrer\">Real-time Personalization using Embeddings for Search Ranking at Airbnb</a> - Airbnb在实时搜索推荐中应用Embedding的经典案例。</li>\n</ol>\n<blockquote>\n<p>🧠 <strong>思考题</strong></p>\n<ol>\n<li>在双塔模型中，为什么用户塔和物品塔的Embedding通常需要保持维度一致？</li>\n<li>除了用户行为序列，你还能想到哪些数据可以用来为商品生成更有意义的Embedding？（例如，商品的文本描述、图片、价格、品牌等）</li>\n<li>你认为<code v-pre>king - man + woman = queen</code>这个类比，在电商场景下的<code v-pre>iPhone - Apple + Huawei ≈ Pura70</code>，其背后的成功本质是什么？</li>\n</ol>\n</blockquote>\n<div class=\"hint-container tip\">\n<p class=\"hint-container-title\">🎉 章节小结</p>\n<p>Embedding是深度学习时代推荐系统的基石，它将离散的ID世界转化为连续的语义空间。从Word2Vec的&quot;上下文定义语义&quot;到Item2Vec的&quot;共现定义关联&quot;，Embedding让机器第一次拥有了理解事物内在联系的能力。无论是召回阶段的相似性搜索，还是排序阶段的特征输入，Embedding都扮演着不可替代的角色。它不仅解决了稀疏性和冷启动问题，更重要的是为推荐系统构建了一个可计算的&quot;语义宇宙&quot;。</p>\n</div>\n<hr>\n<blockquote>\n<p>&quot;You shall know a word by the company it keeps.&quot; —— John Rupert Firth<br>\n在推荐系统的世界里，你也将通过一个物品的&quot;伙伴&quot;来了解它的本质。</p>\n</blockquote>\n","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"在推荐系统的远古时代，计算机眼里的世界是离散且无情的。它如何识别一个商品？答案是：**One-Hot编码**。\n\n想象一个拥有100万商品的大型电商平台：\n- **iPhone 14** 的编码可能是 `[0, 0, ..., 1, ..., 0, 0]` (在第8888个位置是1)\n- **华为 Mate 60** 的编码可能是 `[0, 1, ..., 0, ..., 0, 0]` (在第12345个位置是1)\n- **一本《三体》** 的编码可能是 `[0, 0, ..., 0, ..., 1, 0]` (在第54321个位置是1)\n\n这套系统有两个致命缺陷：\n1.  **语义的真空**：从数学上看，`iPhone 14` 和 `华为 Mate 60` 的向量是完全正交的，它们的距离和 `iPhone 14` 与 `三体` 是一样远的。计算机完全是个\"脸盲\"，无法理解商品之间的任何关联。\n2.  **维度的诅咒**：向量巨大且极其稀疏，这对于存储和计算都是一场灾难。\n\n为了解决语义问题，人们尝试了**人工打标签**，就像给每个商品都办一张\"身份证\"（标签：电子产品、手机、Apple...）。但这太耗费人力，且标签的粒度和质量都难以控制。\n\n世界需要一场革命，一场能让计算机自动理解万物关联的革命。\n\n## 创世纪：Word2Vec 与\"上下文\"的神谕 ✨\n\n革命的曙光来自看似毫不相关的NLP（自然语言处理）领域。2013年，Google的Word2Vec带来了神谕：\n\n> **一个词的意义，由其周围的词来定义 (Context is King)。**\n\n这个思想简单却颠覆认知。比如，在海量文本中：\n- \"国王\" 常常和 \"王后\"、\"权力\"、\"国家\" 一起出现。\n- \"皇帝\" 也常常和 \"皇后\"、\"权力\"、\"国家\" 一起出现。\n\n那么，模型就能自动推断出：\"国王\"和\"皇帝\"的语义是相近的。\n\nWord2Vec做的，就是把每个词语，从高维稀疏的One-Hot向量，**\"降维打击\"** 成一个低维（比如300维）的稠密向量——这，就是 **Embedding**。\n\n## \"语义宇宙\"：Embedding空间的奇妙法则 🌌\n\n这个由Embedding向量构成的低维空间，如同一个\"语义宇宙\"，遵循着奇妙的物理法则：\n\n#### 法则一：距离代表相似性\n在这个宇宙里，语义相近的物体，它们的\"空间坐标\"（Embedding向量）也相互靠近。\n- `distance(国王, 皇帝)` → 很近\n- `distance(国王, 香蕉)` → 很远\n\n#### 法则二：方向蕴含关系\n向量的方向和差异，竟然蕴含了抽象的语义关系。最经典的例子就是：\n> `vector(国王) - vector(男性) + vector(女性) ≈ vector(王后)`\n\n这意味着，从\"国王\"的位置，沿着\"从男性指向女性\"的方向移动，你就能到达\"王后\"的附近！这个简单的数学运算，打开了语义计算的大门。\n\n## 从文本到推荐：Item2Vec 的\"思想迁跃\" 🚀\n\n推荐系统的大师们敏锐地意识到，这个思想可以完美地\"迁跃\"到电商领域：\n\n> **一个商品的意义，由和它一起被浏览/购买的商品来定义。**\n\n于是，Item2Vec诞生了：\n- **语料库 (Corpus)**：不再是文章句子，而是用户的行为序列（如点击序列、购买序列）。\n- **单词 (Word)**：变成了商品ID。\n\n一个用户的购买序列 `[iPhone, AirPods, Apple Watch]` 就相当于一个句子。通过学习大量的这类\"句子\"，模型自动学会了：\n- `Embedding(iPhone)` 和 `Embedding(AirPods)` 在空间中应该很近。\n- `vector(iPhone) - vector(Apple) + vector(小米) ≈ vector(小米手机)` 这种品牌和产品的关系。\n\n**Embedding，第一次赋予了每个商品独一无二、可计算的\"灵魂\"。**\n\n## 现代应用：Embedding 如何驱动推荐系统？ 🎯\n\n在现代推荐系统中，Embedding早已成为水和电一样的基础设施，其应用贯穿始终。\n\n### 阶段一：召回 —— 在亿万商品中\"物以类聚\"\n\n当一个用户来访时，我们如何从亿万商品中，快速找到他可能感兴趣的几千个候选者？答案就是利用Embedding进行高效的相似性搜索。\n\n这通常通过**双塔模型 (Two-Tower Model)** 实现：\n- **用户塔 (User Tower)**：将用户的ID、历史行为序列等信息，通过一个网络（如DNN、RNN）计算出该用户的实时`User Embedding`。\n- **物品塔 (Item Tower)**：离线计算好所有商品的`Item Embedding`。\n\n推荐过程就变成了：拿着实时的`User Embedding`，去庞大的`Item Embedding`宇宙中，通过**近似最近邻搜索 (ANN)** 技术（如Faiss、HNSW），光速般地找出与用户向量最相似的Top-K个商品。\n\n### 阶段二：排序 —— 让Embedding成为模型的\"眼睛\"\n\n召回阶段选出的几千个商品，还需要一个更精细的模型（即**精排模型**，如DeepFM）来精准预估点击率（CTR），决定最终的呈现顺序。\n\n在这里，Embedding扮演了另一个关键角色——**作为特征输入**。\n- **过去**：精排模型看到的输入是 `item_id=8888` 这种无意义的ID。\n- **现在**：模型看到的输入是`item_embedding=[0.12, -0.45, ..., 0.88]` 这样一个充满语义的向量。\n\n这等于给模型装上了一双能理解商品内在含义的\"眼睛\"。它不再是处理一堆ID，而是在处理具有丰富内涵的语义信息，从而能学习到更复杂的交叉模式，做出更精准的判断。\n\n### 拓展应用：解决\"冷启动\"等疑难杂症\n\nEmbedding的魔力不止于此，它还能优雅地解决很多业界难题：\n- **冷启动 (Cold Start)**：一个新商品没有任何行为，如何为它生成Embedding？我们可以利用它的**内容信息**，比如将其文字描述（通过BERT）和图片（通过ResNet）转换成Embedding，给它一个合理的初始\"宇宙坐标\"。\n- **跨领域推荐**：可以将新闻的Embedding和商品的Embedding映射到同一个语义空间，从而实现从新闻偏好到商品推荐的跨越。\n\n## 📖 **延伸阅读**\n1. [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781) - Word2Vec 原始论文，一切开始的地方。\n2. [Item2Vec: Neural Item Embedding for Collaborative Filtering](https://arxiv.org/abs/1603.04259) - 将Embedding思想迁移到推荐系统的经典之作。\n3. [Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba](https://arxiv.org/abs/1803.02349) - 阿里巴巴关于构建工业级大规模商品Embedding的雄文。\n4. [Real-time Personalization using Embeddings for Search Ranking at Airbnb](https://www.kdd.org/kdd2018/accepted-papers/view/real-time-personalization-using-embeddings-for-search-ranking-at-airbnb) - Airbnb在实时搜索推荐中应用Embedding的经典案例。\n\n> 🧠 **思考题**\n> \n> 1.  在双塔模型中，为什么用户塔和物品塔的Embedding通常需要保持维度一致？\n> 2.  除了用户行为序列，你还能想到哪些数据可以用来为商品生成更有意义的Embedding？（例如，商品的文本描述、图片、价格、品牌等）\n> 3.  你认为`king - man + woman = queen`这个类比，在电商场景下的`iPhone - Apple + Huawei ≈ Pura70`，其背后的成功本质是什么？\n\n::: tip 🎉 章节小结\nEmbedding是深度学习时代推荐系统的基石，它将离散的ID世界转化为连续的语义空间。从Word2Vec的\"上下文定义语义\"到Item2Vec的\"共现定义关联\"，Embedding让机器第一次拥有了理解事物内在联系的能力。无论是召回阶段的相似性搜索，还是排序阶段的特征输入，Embedding都扮演着不可替代的角色。它不仅解决了稀疏性和冷启动问题，更重要的是为推荐系统构建了一个可计算的\"语义宇宙\"。\n:::\n\n---\n\n> \"You shall know a word by the company it keeps.\" —— John Rupert Firth  \n> 在推荐系统的世界里，你也将通过一个物品的\"伙伴\"来了解它的本质。","excerpt":"","includedFiles":[],"tasklistId":0,"title":"","headers":[{"level":2,"title":"创世纪：Word2Vec 与\"上下文\"的神谕 ✨","slug":"创世纪-word2vec-与-上下文-的神谕-✨","link":"#创世纪-word2vec-与-上下文-的神谕-✨","children":[]},{"level":2,"title":"\"语义宇宙\"：Embedding空间的奇妙法则 🌌","slug":"语义宇宙-embedding空间的奇妙法则-🌌","link":"#语义宇宙-embedding空间的奇妙法则-🌌","children":[]},{"level":2,"title":"从文本到推荐：Item2Vec 的\"思想迁跃\" 🚀","slug":"从文本到推荐-item2vec-的-思想迁跃-🚀","link":"#从文本到推荐-item2vec-的-思想迁跃-🚀","children":[]},{"level":2,"title":"现代应用：Embedding 如何驱动推荐系统？ 🎯","slug":"现代应用-embedding-如何驱动推荐系统-🎯","link":"#现代应用-embedding-如何驱动推荐系统-🎯","children":[{"level":3,"title":"阶段一：召回 —— 在亿万商品中\"物以类聚\"","slug":"阶段一-召回-——-在亿万商品中-物以类聚","link":"#阶段一-召回-——-在亿万商品中-物以类聚","children":[]},{"level":3,"title":"阶段二：排序 —— 让Embedding成为模型的\"眼睛\"","slug":"阶段二-排序-——-让embedding成为模型的-眼睛","link":"#阶段二-排序-——-让embedding成为模型的-眼睛","children":[]},{"level":3,"title":"拓展应用：解决\"冷启动\"等疑难杂症","slug":"拓展应用-解决-冷启动-等疑难杂症","link":"#拓展应用-解决-冷启动-等疑难杂症","children":[]}]},{"level":2,"title":"📖 延伸阅读","slug":"📖-延伸阅读","link":"#📖-延伸阅读","children":[]}]}}
